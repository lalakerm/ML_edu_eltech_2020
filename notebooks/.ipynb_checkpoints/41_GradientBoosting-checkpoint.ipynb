{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hjOamdgQxa_T"
   },
   "outputs": [],
   "source": [
    "# Импорт необходимых модулей \n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Настройки для визуализации\n",
    "# Если используется темная тема - лучше текст сделать белым\n",
    "TEXT_COLOR = 'black'\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (15, 10)\n",
    "matplotlib.rcParams['text.color'] = 'black'\n",
    "matplotlib.rcParams['font.size'] = 14\n",
    "matplotlib.rcParams['axes.labelcolor'] = TEXT_COLOR\n",
    "matplotlib.rcParams['xtick.color'] = TEXT_COLOR\n",
    "matplotlib.rcParams['ytick.color'] = TEXT_COLOR\n",
    "\n",
    "# Зафиксируем состояние случайных чисел\n",
    "RANDOM_STATE = 0\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfm7nf38BoMd"
   },
   "source": [
    "# Градиентный бустинг\n",
    "\n",
    "По определению бустинг является еще одним методом ансамблирования, как и бэггинг. Отличием является принцип построения и, соответственно, работы модели. В бэггинге мы нехитро разделили набор данных и параллельно учили слабые модели внутри ансамбля. Бустинг в свою очередь является подходом, когда ансамбль составляется последовательно. Слабая модель дополняется другой слабой моделью так, что вторая корректирует ошибки первой - таким образом набирается набор слабых моделей, каждая из которых корректирует предсказания предыдущих.\n",
    "\n",
    "> Самым первым подходом бустинга является алгоритм Adaboost, при этом рассматривать мы его не будем. Вы можете ознакомиться с данным подходом на различных ресурсах интернет. Например [здесь](https://www.youtube.com/watch?v=LsK-xG1cLYA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9RT-6t_ND051"
   },
   "source": [
    "# Сразу переходим к практике!\n",
    "\n",
    "Можно много слов сказать, а мы сразу к делу! Начнем с создания набора данных:\n",
    "\n",
    "> В данной практике мы начнем с задачи регрессии, так как на ее примере дается лучшее представление того, как алгоритм градиентного бустинга учится и работает."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "executionInfo": {
     "elapsed": 3163,
     "status": "ok",
     "timestamp": 1602683339148,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "ckygwsElhbvR",
    "outputId": "9016399d-206c-4862-b4a8-9fe234c2e327"
   },
   "outputs": [],
   "source": [
    "X_data = np.linspace(-1, 7, 200)[:, None]\n",
    "y_data = np.sin(X_data[:,0])*5 + np.random.normal(size=X_data.shape[0])*2 + 5\n",
    "\n",
    "# Посмотрим на данные\n",
    "plt.scatter(X_data[:,0], y_data)\n",
    "plt.grid(True)\n",
    "plt.xlabel('Значение признака ($x$)')\n",
    "plt.ylabel('Истинное значение ($y$)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gkourBm7EE73"
   },
   "source": [
    "По рисунку видно, что данные имеют синусоидальную зависимость. Самое время научиться применять алгоритм градиентного бустинга, чтобы модель смогла определить и повторять данную зависимость в данных!\n",
    "\n",
    "Начнем формирование ансамбля с наиболее простой модели предсказания для регрессии - предсказание среднего значения по вектору истинных значений!\n",
    "\n",
    "> Помните R2? Эта метрика тоже оценивает, насколько лучше модель работает, чем модель, которая всегда предсказывает среднее значение по вектору разметки!\n",
    "\n",
    "Так вот, давайте напишем класс модели, которая во время обучения определяет среднее значение разметки и запоминает его. В качестве предсказания он всегда отвечает в виде среднего значения. Поехали!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hC9R1NQ7XVyD"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "class MeanPredictor:\n",
    "    def __init__(self):\n",
    "        self.y_mean = 0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # TODO - вот здесь нужно получить среднее по y и сохранить\n",
    "        self.y_mean = None\n",
    "\n",
    "    def predict(self, X):\n",
    "        # TODO - предсказание - вектор по размеру количества записей,\n",
    "        #   заполненный средним значением, которое мы сохранили при обучении\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LhFKs7SINqUk"
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "\n",
    "X = np.array([\n",
    "    [0, 1, 2],\n",
    "    [3, 1, 1],\n",
    "    [0, 0, 1],\n",
    "])\n",
    "y = np.array([0, 1, 2])\n",
    "\n",
    "mean_model = MeanPredictor()\n",
    "mean_model.fit(X, y)\n",
    "\n",
    "y_pred = mean_model.predict(X)\n",
    "\n",
    "assert len(y_pred.shape) == 1\n",
    "assert y_pred.shape[0] == 3\n",
    "assert np.all(y_pred == np.array([1, 1, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGYSAJxWOSVN"
   },
   "source": [
    "Теперь убедимся на наших данных, что все работает и наша первая слабая модель отлично работает:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "executionInfo": {
     "elapsed": 3104,
     "status": "ok",
     "timestamp": 1602683339154,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "1UnVueKPyecI",
    "outputId": "e4363be2-2257-4fa9-b7aa-16ad47ae6efd"
   },
   "outputs": [],
   "source": [
    "X = X_data\n",
    "y_true = y_data\n",
    "\n",
    "weak_mean = MeanPredictor()\n",
    "weak_mean.fit(X, y_true)\n",
    "\n",
    "y_pred = weak_mean.predict(X)\n",
    "print(y_pred[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovsdoJQDOy8G"
   },
   "source": [
    "Превосходно! Теперь самое время отобразить наши данные, предсказания модели и отклонения (ошибки) модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 616
    },
    "executionInfo": {
     "elapsed": 3078,
     "status": "ok",
     "timestamp": 1602683339155,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "aKnz6_ci7LZ_",
    "outputId": "6c9310ba-5597-499b-973b-4eb4a97f42d5"
   },
   "outputs": [],
   "source": [
    "y_pred_0 = weak_mean.predict(X)\n",
    "y_resid_0 = y_true-y_pred_0\n",
    "\n",
    "X_vis = np.linspace(X_data.min(), X_data.max(), 100)[:, None]\n",
    "y_pred_vis = weak_mean.predict(X_vis)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, sharey=True)\n",
    "ax[0].plot(X_vis, y_pred_vis, 'r--', lw=3)\n",
    "ax[0].scatter(X_data, y_data)\n",
    "ax[0].set_title('Данные и предсказания')\n",
    "ax[0].grid(True)\n",
    "\n",
    "ax[1].scatter(X_data, y_resid_0, color='green')\n",
    "ax[1].set_title('Отклонения')\n",
    "ax[1].grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHWd3QjVPm3q"
   },
   "source": [
    "Можно заметить, что модель действительно предсказывает среднее по даным значение, а отклонения колеблятся вокруг нуля. Для уверенности можем сразу вывести показатель R2, используя реализацию из `sklearn.metrics.r2_score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "executionInfo": {
     "elapsed": 3049,
     "status": "ok",
     "timestamp": 1602683339156,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "fvjeKOD5P3nw",
    "outputId": "8253ddf6-a87c-444b-c5f7-4a6aa555d738"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2_value_0 = r2_score(y_true, y_pred_0)\n",
    "print(f'R2 score for step 0 = {r2_value_0}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTlpZ9Lvl3wv"
   },
   "source": [
    "А еще, чтобы оценивать, как близко отклонения к нулю, мы воспользуемся метрикой MSE (`sklearn.metrics.mean_squared_error`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "executionInfo": {
     "elapsed": 3021,
     "status": "ok",
     "timestamp": 1602683339157,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "agsUQVGnl9kK",
    "outputId": "b25805c5-47b3-474d-8949-a2aeff6549d4"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as mse_score\n",
    "\n",
    "mse_value_0 = mse_score(y_true, y_pred_0)\n",
    "print(f'MSE score for step 0 = {mse_value_0}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijBB7PeoQGNX"
   },
   "source": [
    "О чем это нам говорит? Если вспомните, то R2 показывает значения в диапазоне [0; 1], если модель работает лучше, чем просто предсказание среднего, и в диапазоне (-inf, 0], если хуже. Соответственно, мы предсказываем среднее значение, поэтому 0!\n",
    "\n",
    "> Зачем мы называли переменные с суффиксом `*_0`? Это чтобы определить шаг обучения. Да, да, мы уже начали учить модель бустинга! Просто пока разбираем по шагам - будут индексы шагов, а потом все будет объединено!\n",
    "\n",
    "Продолжение обучения модели бустинга заключается в том, что мы берем отклонения предсказаний первой модели и должны составить такую модель, которая обучится на этих отклонениях в качестве разметки. То есть $y^{<1>} = y-\\hat{y}^{<0>}$, где $y^{<1>}$ - разметка для обучения на шаге 1, а $\\hat{y}^{<0>}$ - предсказания модели на шаге 0.\n",
    "\n",
    "По сути, самая первая слабая модель учится на исходных данных, следующая модель обучается на исходных признаках, но вот вектор истинных значений уже не исходный, а вектор отклонений предсказаний первой модели.\n",
    "\n",
    "> Думаю, и так понятно, что использовать модель, предсказывающую среднюю модель не имеет смысла, так как новая модель учится на отклонениях, а среднее значение отклонений сейчас равно 0. Поэтому бустинг на модель, предсказывающих среднее не получится.\n",
    "\n",
    "Мы будем использовать в качестве слабой модели (но более сильной по сравнению с простым предсказанием среднего) решающее дерево. Мы раньше знакомились с деревьями для решения задачи классификации, но они также могут применяться и для регресии. Строятся они почти также, просто при делении оценивается не индекс Джини или энтропия, а классический MSE. Поэтому мы не будем заострять на этом внимание и просто воспользуемся реализацией `sklearn.tree.DecisionTreeRegressor`.\n",
    "\n",
    "Еще одной особенностью, которой мы воспользуемся, является ограничение глубины дерева при построении. Установим максимальную глубину равную единице. А знаете как зовется такое дерево, в котором всего один узел и два листа? Подумайте =)\n",
    "\n",
    "<details>\n",
    "<summary>Ответ</summary>\n",
    "\n",
    "Пенек!\n",
    "</details>\n",
    "\n",
    "> В данном случае мы ограничиваем глубину деревьев так сильно, чтобы наблюдать пошаговый эффект. В реальности это может являться одним из гиперпараметров при построении модели, если бустинг делается над решающими деревьями. Вам ведь никто не запрещает делать бустинг над линейными моделями =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gBqTCKEZyh4L"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "weak_stump = DecisionTreeRegressor(\n",
    "    max_depth=1,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "weak_stump.fit(X_data, y_resid_0)\n",
    "\n",
    "y_pred_1 = weak_stump.predict(X_data)\n",
    "y_resid_1 = y_data-(y_pred_0+y_pred_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0bhzrioxgllL"
   },
   "source": [
    "Вот таким нехитрым способом происходит обучение пенька. Так как дерево маленькое, давайте отобразим его структуру:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 623
    },
    "executionInfo": {
     "elapsed": 4285,
     "status": "ok",
     "timestamp": 1602683340462,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "B3NCzjuMKQy7",
    "outputId": "cf561f37-bb0a-44bd-f730-d83748fe6519"
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "tree.plot_tree(weak_stump)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xgur2Qfg3RD"
   },
   "source": [
    "Здесь видно, как узел разделяет значения признака по конкретному значению. Давайте теперь посмотрим на то, как наша вторая слабая модель предсказывает:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 616
    },
    "executionInfo": {
     "elapsed": 4261,
     "status": "ok",
     "timestamp": 1602683340466,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "H1VDEeClQA3S",
    "outputId": "21b6e1e2-71e3-48b3-d76f-7e28b6bd85d0"
   },
   "outputs": [],
   "source": [
    "y_pred_1_vis = weak_stump.predict(X_vis)\n",
    "\n",
    "plt.plot(X_vis, y_pred_1_vis, 'r--', lw=3)\n",
    "plt.scatter(X_data, y_resid_0, color='green')\n",
    "plt.grid(True)\n",
    "plt.title('Отклонения и предсказания пня')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IV5ovaNRhUQV"
   },
   "source": [
    "Видно, что модель, хоть и очень плохо, но постаралась предсказать значения, соответсвующие более менее нашим отклонениям.\n",
    "\n",
    "> Для проверки убедитесь, что информация на картинке соответствует информации в структуре дерева.\n",
    "\n",
    "Хорошо, мы видим, что пень постарался решить задачу регрессии по отклонениям, но что же с моделью бустинга? У нас есть модель среднего и теперь пенек, что делать дальше?\n",
    "\n",
    "Ответ очень прост! Просим обе модели сделать предсказание на данных ($X$) и складываем предсказанные значения! Поехали:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "executionInfo": {
     "elapsed": 4235,
     "status": "ok",
     "timestamp": 1602683340467,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "OxN3qvWviDHg",
    "outputId": "901a5b51-399f-4b53-f506-ab3578637d43"
   },
   "outputs": [],
   "source": [
    "y_pred_0 = weak_mean.predict(X)\n",
    "y_pred_1 = y_pred_0 + weak_stump.predict(X)\n",
    "\n",
    "print(y_pred_1[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u1SkEM0biO9D"
   },
   "source": [
    "Такс, а теперь, чтобы убедиться, что стало лучше, посмотрим на значение R2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "executionInfo": {
     "elapsed": 4207,
     "status": "ok",
     "timestamp": 1602683340468,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "RoQoQyugiU9k",
    "outputId": "5cc80b90-d1c0-471a-b63b-43436e66104e"
   },
   "outputs": [],
   "source": [
    "r2_value_1 = r2_score(y_true, y_pred_1)\n",
    "print(f'R2 score for step 1 = {r2_value_1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUuwDYMNiPCq"
   },
   "source": [
    "Таакс, показатели выросли, это говорит о том, что такой подход действительно получился и работает! Но есть небольшая проблемка...\n",
    "\n",
    "Мы сделали два шага и уже есть два объекта, которые вместе представляют собой ансамбль. То есть эти две слабые модели - это уже модель градиентного бустинга!\n",
    "\n",
    "А что если мы захотим иметь в ансамбле 100 моделей? Давайте автоматизировать! Для начала сделаем список, который и будет по сути моделью бустинга:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gz9JPq9Njdef"
   },
   "outputs": [],
   "source": [
    "gb_model = [weak_mean, weak_stump]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BxVsQVTejglR"
   },
   "source": [
    "А теперь напишем функцию предсказания, которая будет на вход получать модель бустинга (как бы список) и делать предсказания:\n",
    "\n",
    "> Еще разок, предсказание модели бустинга состоит в том, чтобы взять предсказания всех моделей в ансамбле как суперпозицию (сумму предсказаний) моделей! Почему это работает? - Разберем чуть позже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FWOf3Dr5jYqx"
   },
   "outputs": [],
   "source": [
    "# TODO - напишите функцию предсказания модели бустинга\n",
    "def predict_gb(gb_model, X):\n",
    "    '''\n",
    "    gb_model - список слабых моделей в ансамбле в порядке обучения\n",
    "    '''\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AzhpzHfDkQLu"
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "# Проведем тест прямо на наших данных!\n",
    "\n",
    "y_pred = predict_gb(gb_model, X_data)\n",
    "\n",
    "assert y_pred.shape[0] == X.shape[0]\n",
    "assert len(y_pred.shape) == 1\n",
    "# Так как мы не добавляли моделей в ансамбль, сейчас предсказания должны сойтись!\n",
    "assert np.all(y_pred == y_pred_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hah7BvBYksFf"
   },
   "source": [
    "Все, уже проще, нам не нужно делать кучу объектов и вызовов `.predict()` - все делается функцией! Переходим к визуальной оценке - сейчас это наиболее важно для понимания!\n",
    "\n",
    "И тут чирканем функцию для отображения, я сам напишу =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AgiMN_uXlCKK"
   },
   "outputs": [],
   "source": [
    "def plot_model(gb_model, X, y): \n",
    "    X_vis = np.linspace(X.min(), X.max(), 100)[:, None]\n",
    "    y_resid = y-predict_gb(gb_model, X)\n",
    "    y_pred_vis = predict_gb(gb_model, X_vis)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, sharey=True, figsize=[10, 5])\n",
    "    ax[0].plot(X_vis, y_pred_vis, 'r--', lw=3)\n",
    "    ax[0].scatter(X, y)\n",
    "    ax[0].set_title('Данные и модель бустинга')\n",
    "    ax[0].grid(True)\n",
    "\n",
    "    ax[1].scatter(X, y_resid, color='green')\n",
    "    ax[1].set_title('Отклонения')\n",
    "    ax[1].grid(True)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 360
    },
    "executionInfo": {
     "elapsed": 4133,
     "status": "ok",
     "timestamp": 1602683340475,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "JHu3Od0-zQL6",
    "outputId": "8bb354ae-a0e0-486e-aee2-e77751117fb6"
   },
   "outputs": [],
   "source": [
    "X = X_data\n",
    "y_true = y_data\n",
    "\n",
    "plot_model(gb_model, X, y_true)\n",
    "\n",
    "y_pred = predict_gb(gb_model, X)\n",
    "mse_value = mse_score(y_true, y_pred)\n",
    "print(f'Current MSE score: {mse_value}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9DoxbU-KliFY"
   },
   "source": [
    "Наконец-то, графики! Что мы тут видим? Модель уже не представляет собой прямую линию, а приближается к данным! И более того, отклонения стали ближе к нулю - MSE стал заметно ниже. О чем это говорит?\n",
    "\n",
    "Да вот о том, что подход, когда мы берем нынешнее предсказание модели бустинга, вычисляем отклонения и передаем их в качестве целевых значений новой слабой модели на обучение - работает! Давайте для уточнения проведем еще пару шагов, чтобы убедиться, что подход действительно работает. Но перед этим напишем функцию обучения и добавления новой модели в ансамбль:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H-7FQD_ujj27"
   },
   "outputs": [],
   "source": [
    "# TODO - напишите функцию обучения слабой модели (пня)\n",
    "def fit_new_weak_model(gb_model, X, y):\n",
    "    return weak_stump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lovFbiFanmPR"
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "# Тут мы опять сразу проверим на наших данных\n",
    "\n",
    "X = X_data\n",
    "y_true = y_data\n",
    "\n",
    "new_model = fit_new_weak_model(gb_model, X, y_true)\n",
    "\n",
    "assert isinstance(new_model, DecisionTreeRegressor)\n",
    "assert np.isclose(np.mean(new_model.predict(X)), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jglpOpfpoee4"
   },
   "source": [
    "Отлично! А теперь давайте напишем цикл из 20 итераций, в котором будем обучать нашу модель бустинга и отображать как метрики, так и графики! Поехали:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 9619,
     "status": "ok",
     "timestamp": 1602683346015,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "A_RkAfXyoBG_",
    "outputId": "fdf8db6a-7843-4151-94c8-77a64b79bf60"
   },
   "outputs": [],
   "source": [
    "X = X_data\n",
    "y_true = y_data\n",
    "\n",
    "for i in range(20):\n",
    "    print(f'Step {i}')\n",
    "    new_model = fit_new_weak_model(gb_model, X, y_true)\n",
    "    gb_model.append(new_model)\n",
    "\n",
    "    y_pred = predict_gb(gb_model, X)\n",
    "\n",
    "    r2_value = r2_score(y_true, y_pred)\n",
    "    mse_value = mse_score(y_true, y_pred)\n",
    "\n",
    "    plot_model(gb_model, X, y_true)\n",
    "    print(f'R2: {r2_value} | MSE: {mse_value}')\n",
    "    print('---------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jmi7SqlpreT1"
   },
   "source": [
    "Ого, сколько графиков! Но давайте пройдемся по ним и обратим внимание на то, что с каждой новой итерацией улучшаются показатели метрик, отклонения все ближе становятся к нулю и модель все больше соответствует зависимости в данных! Это успех!\n",
    "\n",
    "Так что нам остается понять? Да именно то, что градиентный бустинг так и работает! Пока кратко:\n",
    "- начинаем с самой простой модели предсказания среднего значения (константа);\n",
    "- каждая новая модель в ансамбле учится на отклонениях предсказания модели бустинга;\n",
    "- предсказание модели происходит путем суммирования предсказаний всех моделей (это относится к регрессии).\n",
    "\n",
    "То есть идея подхода в том, что мы обучили модельку, поняли, насколько отклоняются предсказания от истины и задача новой модельки в ансамбле - подкорректировать/помочь модели путем исправления ошибок. Фактически, **новая модель в ансамбле учится исправлять ошибки ранее обученных моделек**. Так и получается, что мы *бустим* нашу модель, пока не получим норм результат.\n",
    "\n",
    "И можно сказать, что такая модель шикарно работает и нет ей аналогов и мы нашли инструмент для любого случая! Отчасти - правда, так как на том же кагле разновидности именно такого подхода чаще всего выигрывают =)\n",
    "\n",
    "Но и тут есть ложка дегтя =( А выяснить в чем дело мы предлагаем вам! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WbFwYvx_tOkP"
   },
   "source": [
    "## Задание\n",
    "\n",
    "Попробуйте разделить наш датасет на два набора (обучение и валидация) и обучить модель на 500 итераций, при этом сохранять значение R2 на каждой итерации и потом отобразить график. Посмотрите, что происходит с графиком R2 и напишите выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 596
    },
    "executionInfo": {
     "elapsed": 9596,
     "status": "ok",
     "timestamp": 1602683346018,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "VHbxMs9DtTCB",
    "outputId": "f75d5d8f-7664-4e55-a54f-b318a7639f61"
   },
   "outputs": [],
   "source": [
    "# TODO - вот здесь пишем код разделения данных, обучения,\n",
    "#   оценки и отображения графика R2\n",
    "\n",
    "\n",
    "\n",
    "# NOTE - создайте новую модель, не используйте старый gb_model\n",
    "gb_model = [] \n",
    "r2_values_train = []\n",
    "r2_values_test = []\n",
    "\n",
    "# А вот здесь пишем код создания новых пеньков и обучения =)\n",
    "# Не забудьте заносить в список R2 метрику для тестовых данных! \n",
    "\n",
    "\n",
    "# Визуализация обучения - уже написано\n",
    "plt.figure(figsize=[10, 10])\n",
    "plt.plot(r2_values_train, 'b', label='Train')\n",
    "plt.plot(r2_values_test, 'k', label='Test')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4O0xACTtYQL"
   },
   "source": [
    "TODO - А вот здесь пишем выводы!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uJJbSi8msxFO"
   },
   "source": [
    "\n",
    "<details>\n",
    "<summary>Подсказка</summary>\n",
    "\n",
    "Если вы не заметили переобучения, посмотрите еще разок =)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4e71afiptc5L"
   },
   "source": [
    "# Формальная часть вопроса\n",
    "\n",
    "Когда мы поняли всю идею подхода, пора чуток коснуться, откуда корни растут! Еще раз спасибо великой математике за это!\n",
    "\n",
    "> Мы тут предсказание модели обозначим через $h(X)$, чтобы не путать с параметрами $W$ все время."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rhAKv1ytuMFH"
   },
   "source": [
    "## Причем тут градиенты?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJGgqOvJuOVi"
   },
   "source": [
    "Начнем наш рассказ с воспоминаний о том, что очень тесно связано с обучением, когда речь идет о методах оптимизации (градиентном спуске) - функции потерь! Фактически, это метрика, но только она обязана быть дифференцируемой. Так как у нас задача регрессии, то метрика как обычно $MSE$ (и как обычно, в виде функции потерь с двоечкой):\n",
    "\n",
    "$$\n",
    "J(\\hat{y}) = MSE = \\frac{1}{2*n}\\sum_{n} (y-\\hat{y})^2\n",
    "$$\n",
    "\n",
    "> А вы тоже заметили, что термин *отклонения* по ходу обучения бустинга и термин *отклонения* в \"*сумма квадратов отклонений*\" очень похожи?\n",
    "\n",
    "Так вот вы не поверите, но все эти действия, что мы делали до этого - мы учили модель градиентным спуском! Давайте начнем как начинали и вспомним самую первую модель, почему мы выбрали именно среднее?\n",
    "\n",
    "По сути, задачу обучения модели можно записать так:\n",
    "$$\n",
    "h(X) = {argmin}_\\hat{y} J(\\hat{y})\n",
    "$$\n",
    "> В этом обозначении мы стараемся найти *такое* предсказание $\\hat{y}$, чтобы получить *минимум функции потерь*.\n",
    "\n",
    "то есть, предсказания модели должны соответствовать минимуму функции потерь. Мы в качестве самой простой модели (самой первой) хотим предсказывать константу, поэтому функция потерь будет выглядеть так:\n",
    "$$\n",
    "J(\\hat{y}) = \\frac{1}{2*n} [(y^{(1)}-\\hat{y}^{(1)})^2 + (y^{(2)}-\\hat{y}^{(2)})^2 + \\dots + (y^{(n)}-\\hat{y}^{(n)})^2]\n",
    "$$\n",
    "\n",
    "Так как предсказание - константа, то $\\hat{y}^{(1)} = \\hat{y}^{(2)} = \\dots = \\hat{y}^{(n)}$. Все сводится к тому, чтобы найти $\\hat{y}$, которое даст минимум функции потерь. Давайте построим график на наших данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 457
    },
    "executionInfo": {
     "elapsed": 29753,
     "status": "ok",
     "timestamp": 1602683366220,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "9uvJqlCytYB8",
    "outputId": "dfa3c72d-1b74-41db-c07d-9c78576e3931"
   },
   "outputs": [],
   "source": [
    "y_pred_vis = np.linspace(2, 7, 30)\n",
    "loss_values = []\n",
    "\n",
    "for y_pred_val in y_pred_vis:\n",
    "    loss_value = mse_score(y_true, np.full_like(y_true, y_pred_val))/2\n",
    "    loss_values.append(loss_value)\n",
    "\n",
    "plt.figure(figsize=[10, 7])\n",
    "plt.plot(y_pred_vis, loss_values)\n",
    "plt.grid(True)\n",
    "plt.ylabel('Loss ~ $J(\\hat{y})$')\n",
    "plt.xlabel('$\\hat{y}$')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jhTbtFLC6cij"
   },
   "source": [
    "То есть по графику мы видим, что в зависимости от значения константного предсказания мы получаем то или иное значение функции потерь. Но как найти минимум? Причем мы меняем всего одно число. Эврика! Найдем производную функции потерь и приравняем ее к нулю! Поехали!\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\hat{y}} J(\\hat{y}) = \n",
    "\\frac{\\partial}{\\partial \\hat{y}} \\frac{1}{2*n}\\sum_{n} (y-\\hat{y})^2 = \n",
    "\\frac{1}{2*n}\\sum_{n} \\frac{\\partial}{\\partial \\hat{y}} (y-\\hat{y})^2\n",
    "$$\n",
    "\n",
    "Осталось определить производную квадрата отклонений:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\hat{y}} (y-\\hat{y})^2 = 2*(y-\\hat{y}) \\frac{\\partial}{\\partial \\hat{y}} (y-\\hat{y}) = -2*(y-\\hat{y})\n",
    "$$\n",
    "\n",
    "В результате получаем итог:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\hat{y}} J(\\hat{y}) = \n",
    "\\frac{-1}{n}\\sum_{n} (y-\\hat{y})\n",
    "$$\n",
    "\n",
    "> Обратите внимание, мы тут не по весам производную ищем, а по значению предсказываемой константы =)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YxaNf4Cm7yLY"
   },
   "source": [
    "Так значит лучшая константа, которая выбирается для начала создания модели будет:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\hat{y}} J(\\hat{y}) = \n",
    "\\frac{-1}{n} [(y^{(1)}-\\hat{y}) + (y^{(2)}-\\hat{y}) + \\dots \n",
    "+ (y^{(n)}-\\hat{y})] = 0\n",
    "$$\n",
    "\n",
    "А это приводит нас выводу, что:\n",
    "$$\n",
    "\\frac{1}{n} [y^{(1)} + y^{(2)} + \\dots \n",
    "+ y^{(n)}] = \\hat{y}\n",
    "$$\n",
    "\n",
    "> Если что, так как все значения предсказания равны константе, я просто перенес все вправо.\n",
    "\n",
    "Воу, так вот как получилось, что лучше всего в качестве начальной модели выбрать среднее по истинным значениям? Круто!\n",
    "\n",
    "А еще мы вычислили градиент функции потерь по значению предсказания, что тоже очень полезно!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S3TLw4d_8wet"
   },
   "source": [
    "## Продолжаем мучать градиент"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-pek922q8zVY"
   },
   "source": [
    "Но на этом наш разбор не закончен, мы только разобрались, почему среднее значение в качестве первой модели - лучший вариант. Пора перейти к вопросу, как так вышло, что новые модели учатся на отклонениях старых?\n",
    "\n",
    "Вводная, мы уже поняли, как получить модель $h^0(X)$, так что сейчас сидим и думаем, что делать на этом шаге.\n",
    "\n",
    "Раньше мы пользовались градиентом, когда хотели менять веса в модели, чтобы уменьшить ту же самую функцию потерь. Выглядело это все таким образом:\n",
    "$$\n",
    "W \\leftarrow W - \\alpha \\frac{\\partial J(W)}{\\partial W}\n",
    "$$\n",
    "\n",
    "То есть градиентом вы выясняли, как надо поменять веса модели, чтобы уменьшить значение функции потерь (а значит улучшить работу модели) и обновляли веса. \n",
    "\n",
    "Теперь давайте подумаем над этим обозначением:\n",
    "$$\n",
    "\\frac{\\partial J(h(X))}{\\partial h(X)}\n",
    "$$\n",
    "\n",
    "Такая производная говорит нам о том, как надо **поменять модель**, чтобы изменить значение функции потерь. По сути, можно было бы сделать так:\n",
    "$$\n",
    "h(X) \\leftarrow h(X) - \\frac{\\partial J(h(X))}{\\partial h(X)}\n",
    "$$\n",
    "\n",
    "> В формуле опущен $\\alpha$, мы его обсудим позже.\n",
    "\n",
    "Получается, что для улучшения нам надо просто прибавить какие-то числа? Не совсем так просто. Когда мы обновляли веса, которые являются числами, то мы и прибавляли числа. Тут - **модели**, а это не числа, а функции/алгоритмы и их не просто \"вычислить\" как числа. \n",
    "\n",
    "Но не все потеряно! Мы же можем обучить новую модель, которая как раз и будет являться тем, что мы можем отнимать/прибавлять по отношению к другим моделям! То есть, нашей задачей является обучить модель предсказывать $-\\frac{\\partial J(h(X))}{\\partial h(X)}$!\n",
    "\n",
    "Подводя итог, если мы обучим некоторую модель $s^1(X)$ предсказывать значения $-\\frac{\\partial J(h(X))}{\\partial h(X)}$, то потом можем сделать так:\n",
    "$$\n",
    "h^1(X) = h^0(X) + s^1(X)\n",
    "$$\n",
    "\n",
    "и получить новую модель, которая (по идее) должна быть лучше!\n",
    "\n",
    "Таакс, а теперь самое сладкое! Вспоминаем, что производная функции потерь $MSE$ по предсказываемому значению - это просто отрицательный остаток:\n",
    "$$\n",
    "-\\frac{\\partial J(h(x))}{\\partial h(x)} = (y-h^0(X))\n",
    "$$\n",
    "> Тут нет знака суммы, потому что мы сейчас рассматриваем каждую запись отдельно.\n",
    "\n",
    "Вот таким вот образом новая модель $s^1(X)$ пытается обучиться предсказывать остатки, чтобы в результате добавления этой модели в ансамбль это уменьшило функцию потерь!\n",
    "\n",
    "Вот такая история!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bhd4sQGaaV_t"
   },
   "source": [
    "## То, о чем не поговорили\n",
    "\n",
    "И на этом моменте появляется **небольшое признание**, после обучения новой модели, чтобы ее прибавить к старым, делается еще один шаг. Принцип построения и предсказания остается примерно таким же. При добавлении модели мы делаем не так:\n",
    "$$\n",
    "h^1(X) = h^0(X) + s^1(X)\n",
    "$$\n",
    "\n",
    "а вот так:\n",
    "$$\n",
    "h^1(X) = h^0(X) + \\alpha * s^1(X)\n",
    "$$\n",
    "\n",
    "\n",
    "$\\alpha$ - это **коэффициент модели**! Этот коэффициент вычисляется путем минимизации по следующему принципу:\n",
    "$$\n",
    "\\alpha = {argmin}_\\alpha J(h^0(X)+\\alpha*s^1(X))\n",
    "$$\n",
    "\n",
    "То есть нам надо подобрать такой корректирующий коэффициент для новой модели, который будет давать минимум функции потерь с ним.\n",
    "\n",
    "> Для дерева мы каждому листу свой коэффициент присваиваем.\n",
    "\n",
    "В ходе обучения слабой модели (решающее дерево для регрессии) используется функция потерь MSE, как и для обучения всего бустинга. Этот коэффициент требуется, когда алгоритмы используют разные функции потерь для обучения.\n",
    "\n",
    "> Для данного случая можете проверить - коэффициент будет все время равен единице.\n",
    "\n",
    "Давайте разберемся, для какого случая это будет применимо. Что если наша функция потерь не MSE, а MAE:\n",
    "$$\n",
    "J(h(X)) = |y-h(X)|\n",
    "$$\n",
    "\n",
    "Тогда отрицательная производная этой функции будет следующая:\n",
    "$$\n",
    "-\\frac{\\partial J(h(X))}{\\partial h(X)} = sign(y-\\hat{y})\n",
    "$$\n",
    "\n",
    "То есть производная равна +1 или -1. В таком случае \"отклонения\" будут выглядеть так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UCYxwGKaXYrf"
   },
   "outputs": [],
   "source": [
    "def plot_model_mae(gb_model, X, y):\n",
    "    X_vis = np.linspace(X.min(), X.max(), 100)[:, None]\n",
    "    y_resid = np.sign(y-predict_gb(gb_model, X))\n",
    "    y_pred_vis = predict_gb(gb_model, X_vis)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, sharey=True, figsize=[10, 5])\n",
    "    ax[0].plot(X_vis, y_pred_vis, 'r--', lw=3)\n",
    "    ax[0].scatter(X, y)\n",
    "    ax[0].set_title('Данные и модель бустинга')\n",
    "    ax[0].grid(True)\n",
    "\n",
    "    ax[1].scatter(X, y_resid, color='green')\n",
    "    ax[1].set_title('Отклонения')\n",
    "    ax[1].grid(True)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "executionInfo": {
     "elapsed": 30734,
     "status": "ok",
     "timestamp": 1602683367245,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "0gK1XxA-TobS",
    "outputId": "cca203d9-1137-45c3-9110-5776813007f6"
   },
   "outputs": [],
   "source": [
    "X = X_data\n",
    "y_true = y_data\n",
    "\n",
    "start_model = MeanPredictor()\n",
    "start_model.fit(None, y_true)\n",
    "gb_model = [start_model]\n",
    "\n",
    "plot_model_mae(gb_model, X, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xkpdA4cVQHH"
   },
   "source": [
    "Как видите, отклонения уже не совсем \"отклонения\", а просто знак отклонения, поэтому эта часть, когда мы вычисляем отрицательную производную функции потерь, называется **псевдо-остатки**. Реальными остатками они являлись, когда мы использовали MSE, но такое случается не всегда, поэтому и появляется приставка *псевдо*.\n",
    "\n",
    "Теперь, зачем тут коэффициент $\\alpha$? А посмотрите, когда новая модель будет учиться на псевдо-остатках, то они не будут видеть истинных значений отклонений, а только знак. Тогда модель научится определять сторону, в которую нужно направлять коррекцию, но не будет знать насколько. Для этого и добавляется коэффициент модели (в случае дерева - каждому листу), чтобы подкорректировать *масштаб* предсказаний коррекции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dsK_5xvvXnh7"
   },
   "outputs": [],
   "source": [
    "def fit_new_weak_model_mae(gb_model, X, y):\n",
    "    y_pred = predict_gb(gb_model, X)\n",
    "    y_resid = np.sign(y-y_pred)\n",
    "\n",
    "    weak_stump = DecisionTreeRegressor(\n",
    "        max_depth=1,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "\n",
    "    weak_stump.fit(X, y_resid)\n",
    "\n",
    "    return weak_stump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "executionInfo": {
     "elapsed": 30706,
     "status": "ok",
     "timestamp": 1602683367247,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "4L1vw0iiWwrj",
    "outputId": "652e5806-cea1-40ec-c6a9-7b06eda2dd55"
   },
   "outputs": [],
   "source": [
    "X = X_data\n",
    "y_true = y_data\n",
    "\n",
    "start_model = MeanPredictor()\n",
    "start_model.fit(None, y_true)\n",
    "gb_model_mae = [start_model]\n",
    "\n",
    "new_model = fit_new_weak_model_mae(gb_model_mae, X, y_true)\n",
    "gb_model_mae.append(new_model)\n",
    "\n",
    "plot_model_mae(gb_model_mae, X, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "executionInfo": {
     "elapsed": 30686,
     "status": "ok",
     "timestamp": 1602683367248,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "kGnkA3mnYYGu",
    "outputId": "08624193-d6cd-434c-932a-eb6d1922f878"
   },
   "outputs": [],
   "source": [
    "# А теперь для примера отобразим обучение с MSE\n",
    "start_model = MeanPredictor()\n",
    "start_model.fit(None, y_true)\n",
    "gb_model_mse = [start_model]\n",
    "\n",
    "new_model = fit_new_weak_model(gb_model_mse, X, y_true)\n",
    "gb_model_mse.append(new_model)\n",
    "\n",
    "plot_model(gb_model_mse, X, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7IJCGQbX2pi"
   },
   "source": [
    "Как видите, модель с функцией отличной от MSE (попробовали MAE) обучилась на остатках и размах результата не такой, как в случае использования MSE. Вся причина в том, что при использовании MSE остатки имеют масштаб не +/- 1, а полноценные ошибки, так что это помогает модели обучаться.\n",
    "\n",
    "К чему это все? Функция потерь была взята MAE для примера, но при этом часто функция может быть совсем другой (не MSE), поэтому надо добавлять коэффициент каждой новой модели, чтобы мастабировать до корректных значений.\n",
    "\n",
    "Здесь мы коэффициент искать не будем, так как это отдельный случай, но вы можете попробовать сделать это сами или использовать готовые инструменты реализации бустинга."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WN7Z2DosRFS_"
   },
   "source": [
    "На этом моменте мы попробовали и освоили алгоритм градиентного бустинга в базовом варианте! \n",
    "\n",
    "* Обратите внимание, что модель бустинга отлично подстраивается под данные, при этом не имея регуляризации, очень подвержена переобучению!\n",
    "\n",
    "* Тем не менее данный ансамблевый метод является очень распространенным и широко используется в работе с данными!\n",
    "\n",
    "Главное, как и с любым другим инструментом, понимать, как он работает, а уже с конкретными реализациями и их тонкосятми (как, например, [CatBoost](https://catboost.ai), [XGBoost](https://xgboost.readthedocs.io/en/latest/), [LightGBM](https://lightgbm.readthedocs.io/en/latest/)) можно разобраться уже по ходу дела! Превосходно!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ulihuzw5Qqpi"
   },
   "source": [
    "# Еще ресурсы\n",
    "\n",
    "Мы неплохо постарались и разобрались с основами градиентного бустинга! И всё-таки, знаниям нет предела и я крайне рекомендую ознакомиться с, например, отличными видео с объяснениями и рассказом по этой теме от StatQuest:\n",
    "- Gradient Boostring - Regression [part 1](https://www.youtube.com/watch?v=3CC4N4z3GJc), [part 2](https://www.youtube.com/watch?v=2xudPOBz-vs)\n",
    "- Gradient Boostring - Classification [part 3](https://www.youtube.com/watch?v=jxuNLH5dXCs), [part 4](https://www.youtube.com/watch?v=StWY5QWMXCw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWbBoxrKVH4t"
   },
   "source": [
    "# Понижение размерности данных\n",
    "\n",
    "Раз уж заговорили о серьезном, то и обсудим сразу интересную методику под названием *понижение размерности*, которую можно активно применять в решениях задач! Это небольшой шаг в сторону от бустинга, но тем не менее полезно знать и использовать!\n",
    "\n",
    "Работа с реальными данными часто вынуждает анализировать множества признаков, более двух, что не позволяет отобразить данные графически и сделать визуальную оценку. С одной стороны, большое количество признаков позволяет делать прогнозы более точно, а расширение количества признаков за счет новой информации или feature engineering может положительно сказаться на предиктивной способности модели.\n",
    "\n",
    "> Предиктивная способность модели - объединение обобщающей способности модели с ее точностью предсказания. Можно назвать *корректность модели*.\n",
    "\n",
    "Но ложкой дегтя здесь является то, что не все модели положительно реагируют на слишком большое количество признаков. Так, например, деревья, деля данные лишь по одному признаку за раз, не могу создавать обобщающие гипотезы среди нескольких признаков, как, например, это делает линейная регрессия. Поэтому, мы рассмотрим подход на основе *обучения без учителя*, который позволяет из набора признаков сделать меньший набор признаков с как можно меньшими потерями информации.\n",
    "\n",
    "Метод Principal Component Analysis (PCA) является одним из методов понижения размерности. Все методики понижения размерности нацелены на уменьшение количества признаков путем выделения новых признаков (чаще всего не имеющих физического представления) меньшего количества.\n",
    "\n",
    "Метод PCA основан на принципе нахождения новых ортогональных векторов, на которые проецируются данные.\n",
    "\n",
    "![Пример PCA](https://dinhanhthi.com/img/post/ML/PCA/pca-1.jpg)\n",
    "\n",
    "Например в случае картинки для двух признаков (2D) ищется такой вектор, чтобы проекция на него имела наименьшую ошибку. Таким образом происходит понижение размерности от двух признаков к одному (2D -> 1D).\n",
    "\n",
    "Такая линия напоминает работу с линейной регресией, не так ли? Все верно, это очень похоже! Только в качестве метрики для линейной регрессии мы старались сделать расстояние от линии до точек минимальным, а так как $y-\\hat{y}$, то расстояние от точки до линии было вертикальной линией - это называется *отклонение предсказания / ошибка предсказания*. Пример на картинке ниже:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 630
    },
    "executionInfo": {
     "elapsed": 1059,
     "status": "ok",
     "timestamp": 1602794491413,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "OfOMQei8_Nk5",
    "outputId": "68747f6d-a169-45c5-aacc-e1a595e52012"
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-1.2,1.2,20)\n",
    "y = x\n",
    "dy = (np.random.rand(20)-0.5)\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.scatter(x,y+dy)\n",
    "plt.vlines(x,y,y+dy)\n",
    "plt.ylabel('$y$')\n",
    "plt.xlabel('$x$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_XW2P0F9zHh"
   },
   "source": [
    "**Ошибка проекции** - это тоже расстояние от точек до линии, но только это расстояние отсчитывается наикратчайшим, то есть перпендикулярным вектору, на который делается проекция (как показано на картинке про PCA).\n",
    "\n",
    "То есть мы из двух признаков делаем проекцию на вектор и получаем один (вместо двух). Так последовательно сводя по два признака к одному мы уменьшаем размерность данных (количество признаков), при этом зачастую теряя их физический смысл.\n",
    "\n",
    "> Суть в том, что найдя вектор PCA между \"площадью дома\" и \"количеством соседей\" в данных, мы можем попытаться выразить это через какое-то взаимосвязанное определение, но чаще всего это проще назвать PCA компонентой.\n",
    "\n",
    "Чем полезен такой подход? Помимо того, что у нас появляется возможность отобразить многомерные данные, также мы можем получить улучшение работы модели, так как мы уменьшаем влияние эффекта под названием **проклятье размерности**.\n",
    "\n",
    "> Проклятье размерности (Curse of Dimensionality) - термин, который описывает сложности работы с данными, в которых [слишком много признаков](http://www.machinelearning.ru/wiki/index.php?title=%D0%9F%D1%80%D0%BE%D0%BA%D0%BB%D1%8F%D1%82%D0%B8%D0%B5_%D1%80%D0%B0%D0%B7%D0%BC%D0%B5%D1%80%D0%BD%D0%BE%D1%81%D1%82%D0%B8).\n",
    "\n",
    "Давайте теперь посмотрим, как работать с этим методом в `sklearn`. Для примера возьмем датасет по классификации вин:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "executionInfo": {
     "elapsed": 30667,
     "status": "ok",
     "timestamp": 1602683367248,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "FFFvB4ePaO1U",
    "outputId": "3de8129e-36e3-459c-b3cd-105c81c20697"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "\n",
    "wine_data = load_wine()\n",
    "feat_names = wine_data['feature_names']\n",
    "\n",
    "df = pd.DataFrame(wine_data['data'], columns=feat_names)\n",
    "df['CLASS'] = wine_data['target']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 613
    },
    "executionInfo": {
     "elapsed": 31466,
     "status": "ok",
     "timestamp": 1602683368067,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "Jtei05-maQh5",
    "outputId": "03e1ac38-c259-4ebb-c452-61df9b7300e4"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X_df = df[feat_names]\n",
    "y = df['CLASS']\n",
    "target_names = [\n",
    "    'Class0',\n",
    "    'Class1',\n",
    "    'Class2',\n",
    "]\n",
    "\n",
    "# Задается количество признаков, которое хотим получить\n",
    "# Для визуализации понижаем количество признаков до 2х\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X_df)\n",
    "X_pca = pca.transform(X_df)\n",
    "\n",
    "plt.figure()\n",
    "for l, c, m in zip(range(0, 3), ('blue', 'red', 'green'), ('^', 's', 'o')):\n",
    "    plt.scatter(\n",
    "        X_pca[y == l, 0],\n",
    "        X_pca[y == l, 1],\n",
    "        color=c,\n",
    "        label=target_names[l],\n",
    "        alpha=0.8,\n",
    "        marker=m\n",
    "    )\n",
    "\n",
    "plt.legend(target_names)\n",
    "plt.xlabel('PCA_component_0')\n",
    "plt.ylabel('PCA_component_1')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ASAnC7YcaTGT"
   },
   "source": [
    "Таким вот образом мы смогли отобразить данные, которые представляют собой 13 признаков. Мы видим, что Class0 хорошо отделим от остальных данных, а остальные два класса сильно смешались. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-M0JKP9IaVkk"
   },
   "source": [
    "# Задание\n",
    "\n",
    "Попробуйте произвести стандартизацию данных и после этого проверьте, как работает метод PCA после стандартизации (отобразите данные):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jzqvb18LaY5m"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LomRcG2HabOQ"
   },
   "source": [
    "В результате видно, что стандартиация данных очень положительно сказывается на работе метода PCA. После стандартизации и понижения размерности данные стали намного более разделимы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-mEj9kSafhy"
   },
   "source": [
    "# Задание\n",
    "\n",
    "Оцените кросс-валидацией показатель `f1_macro` трех вариантов на данных:\n",
    "- Модель логистической регрессии;\n",
    "- Модель логистической регрессии со стандартизацией;\n",
    "- Модель ЛР после понижения размерности с помощью [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) до 2х компонент;\n",
    "- Модель ЛР со стандартиацией и понижением размерности до 2х компонент;\n",
    "- Модель случайного леса;\n",
    "- Модель случайного леса со стандартиацией и понижением размерности до 2х компонент;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WwfZxNPUaini"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_V5oInmaiKy"
   },
   "source": [
    "# Задание\n",
    "\n",
    "Оцените работу модели Случайного леса со стандартизацией и понижением размерности до количества в диапазоне [2; 13] (всего 12 значений); постройте график в осях (количество компонент; показатель метрики на кросс-валидации);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XyoxLhQza_Yv"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P4fm-j7xak5O"
   },
   "source": [
    "# Задание\n",
    "\n",
    "Разберитесь с методом [TSNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) и сравните влияние стандартизации и работу с методом PCA - визуализируйте данные и проверьте влияние стандартизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cmns5AanbHqa"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0fDTw-HiS1wI"
   },
   "source": [
    "# Задание - выводы\n",
    "\n",
    "Как всегда, ваши выводы - это одна из самых ценных вещей для вас самих, не стесняйтесь записывать даже самые малые наблюдения и умозаключения! Мы вам постараемся помочь вопросами:\n",
    "\n",
    "- Чем отличается принцип работы PCA и tSNE? В чем их назначения? Можно ли оба использовать для понижения размерности?\n",
    "- Какие еще существуют методы понижения размерности?\n",
    "- Чем бустинг отличается от бэгинга?\n",
    "- Почему бустинг - это ансамблевый метод?\n",
    "- Можно ли построить модель бустинга на основе случайных лесов? (Бустинг над бэггингом)? Будет ли в этом смысл?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0qLEnLXw1a1"
   },
   "source": [
    "# Вопросы\n",
    "\n",
    "1. Почему бустинг является методом ансамблирования? \n",
    "2. Что показывает градиент? \n",
    "3. Зачем нужен коэффициент модели? \n",
    "4. Зачем понижать размерность данных? \n",
    "5. Какая структура \"растительности\" используется в AdaBoost? \n",
    "6. Почему модель может быть названа слабой? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8sXo2Es5pNR"
   },
   "source": [
    "# Небольшое заключение\n",
    "\n",
    "На данный момент мы освоили уже некоторое (хорошее) количество различных моделей:\n",
    "- Линейная/логистическая регрессия;\n",
    "- (Если выполняли лабу) KNN/SVM;\n",
    "- Решающие деревья/случайный лес;\n",
    "- Градиентный бустинг.\n",
    "\n",
    "Мы на этом не останавливаемся, но нужно сделать небольшое резюме! Простые модели отлично подходят для решения простых задача, потому что работа с ними имеет меньше шансов на переобучение и другие проблемы. При работе со сложными задачами лучше использовать леса или бустинг. Именно поэтому первым делом **всегда** необходимо анализовать имеющиеся данные и искать возможности их улучшить/очистить/исправить/дополнить. Хорошо обработанные данные могут дать выше процент точности с использованием логистической регрессии, нежели сырые данные с применением леса! Такое случается не всегда, но важно понимать, что данные в большинстве определяют способность модели предсказывать.\n",
    "\n",
    "После того, как данные подготовлены, можно попробовать использовать различные модели и поискать гиперпараметры, которые дадут наилучшую точность. Вот здесь важно помнить об **обобщающей способности модели** и не влететь в переобучение на радостях высоких процентов точности!\n",
    "\n",
    "По результатам работы всегда возвращайтесь к анализу ошибок модели, так как это еще один источник информации! Те же леса могут в явном виде оценивать важности признаков, что позволит сконцентрировать внимание и найти еще дополнительной информации в данных!\n",
    "\n",
    "Теперь о бустинге и бэггинге. Оба подхода являются ансамблевыми, но при этом бустинг своим способом обучения может более точно описывать данные (инкрементно стараемся все больше улучшить модель). Вы в этой практике уже смогли переобучить модель, поэтому важно помнить, что **бустинг подвержен переобучению** больше, нежели леса и подход бэггинга!\n",
    "\n",
    "Так что при разработке модели осознанный выбор лучше делать простым испытанием обоих подходов! Просто берем и ищем лучшие модели обоих подходов и после уже решаем, что в данном случае подходит больше. А для сравнения моделей мы уже узнали про различные показатели метрик. Помимо этого, на принятие решений может повлиять и специфика решаемой задачи.\n",
    "\n",
    "Не бойтесь экспериментировать и узнавать новое! Как правило классические подходы дают хорошие результаты предсказаний, а лучшие достигаются крайне нестандартными способами - творите!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Pr5_GradientBoosting.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
