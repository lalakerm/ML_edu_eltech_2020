{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z7YJYY69vMvQ"
   },
   "outputs": [],
   "source": [
    "# Импорт необходимых модулей \n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Настройки для визуализации\n",
    "# Если используется темная тема - лучше текст сделать белым\n",
    "TEXT_COLOR = 'black'\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (15, 10)\n",
    "matplotlib.rcParams['text.color'] = 'black'\n",
    "matplotlib.rcParams['font.size'] = 14\n",
    "matplotlib.rcParams['axes.labelcolor'] = TEXT_COLOR\n",
    "matplotlib.rcParams['xtick.color'] = TEXT_COLOR\n",
    "matplotlib.rcParams['ytick.color'] = TEXT_COLOR\n",
    "\n",
    "# Зафиксируем состояние случайных чисел\n",
    "RANDOM_STATE = 0\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VBp-xddAvdIv"
   },
   "source": [
    "# Решающие деревья"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4EPQ7pkr-wND"
   },
   "source": [
    "На данный момент мы уже познакомились с основными задачами, которые решаются в обучении с учителем:\n",
    "* определение регрессии\n",
    "* классификация.\n",
    "\n",
    "Более того, мы не только узнали, но и реализовали две модели: линейной и логистической регрессии. Обе модели по своей природе являются линейными, но тем не менее позволяют решать уже очень много различных задач, которые по природе своей сводятся либо к задаче определения регрессии, либо к классификации - мы в этом убедились в лабораторных работах!\n",
    "\n",
    "Но что если в задаче регрессии зависимости перестают быть линейными и становятся сильно нелинейными? Или в классификации уже не получается разделить данные на классы прямыми линиями? Нужен инструмент посерьезнее!\n",
    "\n",
    "В этой практике мы дойдем до рассмотрения модели под названием **Случайный лес (Random Forest)**. Он основывается на более простых моделях под названием **Решающие деревья (Decision Trees)**, их мы тоже рассмотрим!\n",
    "\n",
    "Зачем нам другие модели, если мы уже знаем как минимум две (а может и больше)? Ранее использованные модели достаточно простые и описывают данные таким образом, как и задумывается. То есть линейная регрессия (если не генерировать полиномиальные признаки) описывает данные линейной зависимостью. Аналогично, логистическая регрессия, мы точно знаем, что разделение будет прямой линией. Это дает нам уверенность, что модель точно не переобучится (overfit), возможно, недообучится (underfit), но это проще заметить, так как сразу видно, что у модели низкая точность по показателям. Для определения переобучения надо приложить больше внимательности.\n",
    "\n",
    "> Еще раз уточним, что переобучение плохо тем, что модель перестает видеть зависимость в данных, а начинает ориентироваться на шум и запоминать данные. На новых данных будет мнооого ошибок.\n",
    "\n",
    "Тем не менее, часто зависимости бывают сложные и нелинейные, поэтому дальнейшие новые модели - это уже серьезные инструменты. Их тоже можно настраивать и мы научимся этим пользоваться, а пока помните принцип \"бритвы Оккама\" - не усложняй. Если зависимости в данных простые - сложные модели не нужны. Если всё-таки данные сложные - давайте научимся, как пользоваться уже недетскими штучками!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KOTjlKZhviPi"
   },
   "source": [
    "Решающее дерево - это модель, которая все время что-то выбирает. Для примера взглянем на решающее дерево, которое классифицирует, будем играть в футбол или нет:\n",
    "\n",
    "![Решающее дерево](https://docs.google.com/uc?export=download&id=1jT-uZf7Og09ZpOMKwj4HqSdcraHgNL0e)\n",
    "\n",
    "> Напоминает игру \"20 вопросов\", не так ли? Когда тебе загадали знаменитось и ты пытаешься отгадать за счет вопросов с ответами Да/Нет.\n",
    "\n",
    "Само решающее дерево состоит из узлов (синие ячейки, фиолетовая - корневой узел) и листьев (зеленые ячейки). В машинном обучении применяются бинарные деревья (только Да/Нет).\n",
    "\n",
    "Узлы проверяют значения признаков, листья - это конкретные классы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8pFkWVKS3PzJ"
   },
   "source": [
    "Как происходит предсказание с использованием рещающего дерева? Построенное дерево представляет собой структуру, в которой в каждом узле принимается решение о том, больше или меньше значение определенного признака $m$ чем какой-то порог. Попадая на листья, мы получаем класс предсказания.\n",
    "\n",
    "Например, пускай в наших данных одним из признаков является стоимость, тогда узлом дерева может быть принятие решения \"является ли стоимость больше 10 у.е.\". Таким образом, имея новую запись данных, мы спускаемся по дереву, проверяя признаки на превышение порога, заданного в узле - так выбирается куда идти, в левую или правую ветку.\n",
    "\n",
    "Таким образом, функция предсказания дерева - набор условных проверок, каждая из которых имеет определенное число (порог) для признака, зафиксированного в узле.\n",
    "\n",
    "Для того, чтобы реализовать функцию предсказания, требуется в первую очередь разобраться, как дерево строится и как представляется."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYYGHks450WA"
   },
   "source": [
    "# Решающее дерево для классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ql7_WOAH541p"
   },
   "source": [
    "Для того, чтобы разобраться, как деревья строятся, посмотрим на набор данных из двух признаков (чтобы была возможность визуализации):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 630
    },
    "executionInfo": {
     "elapsed": 2430,
     "status": "ok",
     "timestamp": 1601654555903,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "9j6gBotX6F4G",
    "outputId": "737c27aa-93cc-4872-d746-bec677177622"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X_data, y_data = make_classification(\n",
    "    n_samples=10,\n",
    "    n_features=2, \n",
    "    n_redundant=0,\n",
    "    n_informative=1, \n",
    "    n_clusters_per_class=1,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "pnts_scatter = plt.scatter(X_data[:, 0], X_data[:, 1], marker='o', c=y_data, s=50, edgecolor='k', )\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.grid(True)\n",
    "plt.legend(handles=pnts_scatter.legend_elements()[0], labels=['0', '1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wtBcEskR54vP"
   },
   "source": [
    "Обратите внимание на данные, классы очень хорошо разделены и человек сразу понимает, проведите линию на уровне $x_1=0.5$ и мы получим идеальную классификацию по единственному признаку $x_1$: класс 1, если $x_1 > 0.5$ и класс 1 в ином случае. \n",
    "\n",
    "Попробуйте сформулировать условия как дерево решений:\n",
    "\n",
    "<details>\n",
    "    <summary>Решение</summary>\n",
    "\n",
    "![Картинка](https://docs.google.com/uc?export=download&id=1FdEVwJlMRo4YTdlZzZT9_X03jq2bpQJs)\n",
    "</details>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYs4OQG-BrQU"
   },
   "source": [
    "Как видно из представления решающего дерева, для корректной классификации достаточно дерева с единственным корневым узлом и глубиной равной единице.\n",
    "\n",
    "> Глубина дерева - это количество узлов, уходящих \"вниз\". В примере с игрой в футбол глубина дерева равна трем.\n",
    "\n",
    "Но не забывайте, что это дерево построено нами, а как сделать это алгоритмически на основе данных?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gAt0pRHvC_mA"
   },
   "source": [
    "# Примеси Джини (Gini impurity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YOtb9L1JDEP3"
   },
   "source": [
    "Примеси Джини - это коэффициент, который показывает, сколько в наборе данных примесей. Под \"примесями\" понимается неоднородность классов в данных. То есть, в нашем наборе пять записей с классом 0 и пять - с классом 1. Для понимания, данные без примесей - данные *только* с классом 0 или 1.\n",
    "\n",
    "Формула коэффициента следующая:\n",
    "$$\n",
    "G = \\sum_{k=1}^{K} p(k)*(1-p(k))\n",
    "$$\n",
    "где $K$ - количество классов в данных, $p(k)$ - вероятность выбора класса $k$ из данных.\n",
    "\n",
    "Что такое \"вероятность выбора класса\"? Просто, каков шанс, что выбрав из данных запись, он окажется с этим классом. А как посчитать вероятность выскивания зеленого шарика из корзинки с зелеными и синими шариками, если мы знаем, что в корзине 5 синих и 5 зеленых? Правильно, просто поделить количество зеленых на общее количество!\n",
    "\n",
    "Так и мы тут делаем, посчитаем ручками:\n",
    "$$\n",
    "G = \\frac{5}{10}*(1-\\frac{5}{10}) + \n",
    "\\frac{5}{10}*(1-\\frac{5}{10}) \n",
    "= 0.5*0.5 + 0.5*0.5 = 0.5\n",
    "$$\n",
    "\n",
    "Сколько половинок в этой формуле, ничего не понятно, давайте попробуем более неравномерное разделение? Допустим имеем корзинку из 10 шариков, 2 из них синие, 8 - зеленые. Посчитаем примеси Джини:\n",
    "$$\n",
    "G = \\frac{2}{10}*(1-\\frac{2}{10}) + \n",
    "\\frac{8}{10}*(1-\\frac{8}{10}) \n",
    "= 0.2*0.8 + 0.8*0.2 = 0.32\n",
    "$$\n",
    "\n",
    "И для контраста посчитаем, что если в корзине 10 шариков и все зеленые (хотя мы еще знаем про синие):\n",
    "$$\n",
    "G = \\frac{10}{10}*(1-\\frac{10}{10}) +\n",
    "\\frac{0}{10}*(1-\\frac{0}{10}) = 0\n",
    "$$\n",
    "\n",
    "Вот мы провели расчеты, но что это дало? Первое и самое важное - в первых двух случая есть примеси и, судя по показателю, в первом случае их больше, чем во втором. В третьем случае примесей нет совсем.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3Sv6JuPHU-H"
   },
   "source": [
    "Давайте напишем реализацию этой функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yBxXZOG2HcvZ"
   },
   "outputs": [],
   "source": [
    "# TODO - напишите реализацию функции вычисления Джини\n",
    "def gini_impurity(y):\n",
    "    return gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tH30TfEBILjf"
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "y1 = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])\n",
    "y2 = np.array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
    "y3 = np.array([0, 0, 1, 1, 1, 1, 1, 1, 1, 1])\n",
    "y4 = np.array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
    "\n",
    "assert gini_impurity(y1) == 0.5\n",
    "assert gini_impurity(y2) == 0\n",
    "assert gini_impurity(y3) == 0.32\n",
    "assert gini_impurity(y4) == 0\n",
    "assert gini_impurity(np.array([])) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jhK9RiOtJBkS"
   },
   "source": [
    "Для чего нам нужен этот показатель? Суть решающего дерева заключается в том, что каждым узлом производится раздел пространства на части. То есть, если мы говорим, что узел разделяет по признаку $x_1$ с порогом 0.5, то все пространство правее линии $x_1 = 0.5$ становится классом 1, а все левее этой линии - классом 0. Для проверки напишем первый вариант функции предсказания и построим визуализацию решений модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sPl7lDWuJXDB"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "def predict_v1(X):\n",
    "    # Напишите реализацию функции предсказания\n",
    "    #   решающего дерева с одним узлом\n",
    "    #   разделение по признаку (x1) с порогом 0.5\n",
    "    # *Не забывайте о размерности данных X\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_FYRz78MKUPZ"
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "X = np.array([\n",
    "    [1, 1],\n",
    "    [2, 1],\n",
    "    [0, 1],\n",
    "])\n",
    "\n",
    "assert np.all(predict_v1(X) == np.array([1, 1, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRSSN9ksJjBb"
   },
   "source": [
    "Теперь вернемся к нашим данным и посмотрим, как работает предсказание разделением по единственному признаку:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 613
    },
    "executionInfo": {
     "elapsed": 2737,
     "status": "ok",
     "timestamp": 1601654556254,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "_glfuBa_KnRa",
    "outputId": "e96b8197-b5af-45de-bd92-6993a58270a4"
   },
   "outputs": [],
   "source": [
    "X = X_data\n",
    "y_true = y_data\n",
    "\n",
    "x1_vals = np.linspace(X[:,0].min()-0.5, X[:,0].max()+0.5, 100)\n",
    "x2_vals = np.linspace(X[:,1].min()-0.5, X[:,1].max()+0.5, 100)\n",
    "xx, yy = np.meshgrid(x1_vals, x2_vals)\n",
    "space_X = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "y_pred = predict_v1(space_X)\n",
    "y_pred = y_pred.reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx, yy, y_pred)\n",
    "pnts_scatter = plt.scatter(X[:, 0], X[:, 1], c=y_true, s=50, edgecolor='k')\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.grid(True)\n",
    "plt.legend(handles=pnts_scatter.legend_elements()[0], labels=['0', '1', '2'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6I4rPeJmKrTB"
   },
   "source": [
    "Как видно, реализация дерева с одним узлом разделяет пространство решений на две части. Если добавить еще узлов в дерево, то разделение будет продолжаться прямыми линиями дальше (это мы увидим в дальнейших шагах)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qaZHazY9LP-c"
   },
   "source": [
    "Сейчас важно понять, как нам создать такое разделение автоматически?\n",
    "\n",
    "Для начала, мы же не знаем лучшее разделение из данных - нам надо получить его автоматически. Попробуем три разных порога для разделения данных по признаку $x_1$ (который стоит в колонке 0) и посчитаем примеси Джини каждой части после разделения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 180
    },
    "executionInfo": {
     "elapsed": 2717,
     "status": "ok",
     "timestamp": 1601654556255,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "ggoLR4YzLyqx",
    "outputId": "d9f4e052-4471-42c4-cf47-428bd228852b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "thresholds = [-0.5, 0.5, 1]\n",
    "feature_index = 0\n",
    "\n",
    "X = X_data\n",
    "y_true = y_data\n",
    "\n",
    "gini_full = gini_impurity(y_true)\n",
    "print(f'Gini full: {gini_full} | count: {len(y_true)}')\n",
    "\n",
    "for threshold in thresholds:\n",
    "    print(f'\\tSplit by {threshold}')\n",
    "    split_mask = X[:, feature_index] > threshold\n",
    "    y_true_left = y_true[split_mask]\n",
    "    y_true_right = y_true[~split_mask]\n",
    "    \n",
    "    gini_left = gini_impurity(y_true_left)\n",
    "    gini_right = gini_impurity(y_true_right)\n",
    "\n",
    "    print(f'\\t\\tGini left: {gini_left} | element count: {len(y_true_left)}')\n",
    "    print(f'\\t\\tGini right: {gini_right} | element count: {len(y_true_right)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lo53au3QNQRk"
   },
   "source": [
    "По выводу видно, что после того или иного разделения показатели меняются, но как нам понять, стало лучше или хуже? \n",
    "\n",
    "Давайте возьмем сумму левого и правого показателей! По идее, чем меньше сумма - тем лучше разделение, так как примесей становится меньше (неопределенности класса). Только учтем маленькую особенность, мы каждому значению примеси добавили вес. Так мы получим взвешенную сумму!\n",
    "\n",
    "Вес мы будем вычислять следующим образом, вот у нас есть 10 элементов до разделения, при делении по -0.5 мы получаем ветки с девятью и одним элементом. Значит для левой ветки вес будет $9/10$, а для правой $1/10$. То есть, новый показатель будет равен: $0.9*0.49+0.1*0 = 0.44$.\n",
    "\n",
    "Зачем взвешивать сумму? Маленький показатель примесей - это круто, например 0.1, но вот только если его получили сотней элементов это намного лучше, чем, если он получен десятью элементами. То есть, если много элементов дают маленький показатель примесей, то это более желаемый кейс, чем тот же показатель, но меньшее число элементов.\n",
    "\n",
    "Давайте сделаем расчет в нашем случае:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "executionInfo": {
     "elapsed": 2696,
     "status": "ok",
     "timestamp": 1601654556256,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "dxRo7oXeOAD5",
    "outputId": "038b5eff-50b7-41b5-a6db-b14b653f7bd0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "thresholds = [-0.5, 0.5, 1]\n",
    "feature_index = 0\n",
    "\n",
    "X = X_data\n",
    "y_true = y_data\n",
    "\n",
    "gini_full = gini_impurity(y_true)\n",
    "print(f'Gini full: {gini_full}')\n",
    "\n",
    "for threshold in thresholds:\n",
    "    print(f'\\tSplit by {threshold}')\n",
    "    split_mask = X[:, feature_index] > threshold\n",
    "    y_true_left = y_true[split_mask]\n",
    "    y_true_right = y_true[~split_mask]\n",
    "    \n",
    "    gini_left = gini_impurity(y_true_left)\n",
    "    gini_right = gini_impurity(y_true_right)\n",
    "\n",
    "    print(f'\\t\\tGini left: {gini_left}')\n",
    "    print(f'\\t\\tGini right: {gini_right}')\n",
    "    \n",
    "    weight_left = len(y_true_left)/len(y_true)\n",
    "    weight_right = len(y_true_right)/len(y_true)\n",
    "    weighted_gini = weight_left * gini_left + weight_right * gini_right\n",
    "    print(f'\\t\\tGini after split: {weighted_gini}')\n",
    "    print(f'\\t\\tGini gain: {gini_full-weighted_gini}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xoV2IOU9OhZI"
   },
   "source": [
    "Вывод разницы между исходным показателем и после разделения называется \"прирост Джини\" (Gini Gain). Таким образом, если показатель стал меньше, значит в разделении стало меньше примесей. То есть, чем больше прирост Джини, тем лучше разделение. Крайний случай - ноль. При нуле происходит идеальное разделение между классами.\n",
    "\n",
    "> Не совсем очевидно, но \"прирост Джини\" - это величина уменьшения этого показателя в результате разделения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "epSbZiftPU-m"
   },
   "source": [
    "Таким образом, мы вывели правило, по которому производится оценка того, как выбрано разделение (параметры узла). Что же нам остается? Понять, а как же всё-таки выбирать, по каким признакам и какие пороги ставить?\n",
    "\n",
    "> Не забывайте, что сейчас мы пороги задавали сами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mouxC16ZPvUl"
   },
   "source": [
    "# Выбор лучшего разделения (сплита)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MM1QxI6QPyt3"
   },
   "source": [
    "Ответ: ничего особенного, мы будем делать жадный перебор =)\n",
    "\n",
    "Почему? Да потому что это просто! То есть, суть алгоритма выбора признака для разделения и порога по этому признаку будет заключаться в том, что мы будем перебирать каждый индекс признака и в рамках этого признака - каждую запись в качестве порога.\n",
    "\n",
    "То есть, в качестве псевдокода можно записать так:\n",
    "```\n",
    "Цикл по всем признакам\n",
    "    Цикл по всем записям\n",
    "        Берем значение признака в этой записи в качестве порога\n",
    "        Делаем разделение\n",
    "        Вычисляем примеси Джини\n",
    "        Если прирост Джини больше, чем сохраненный,\n",
    "            то сохранить индекс признака и значение порога,\n",
    "            а также обновить лучшую примесь \n",
    "            и взвешенную сумму примесей\n",
    "```\n",
    "\n",
    "А теперь, реализуйте его в качестве функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T99gTtOD5t3v"
   },
   "outputs": [],
   "source": [
    "def get_best_split(X, y_true):\n",
    "    best_gini_gain = 0\n",
    "    best_gini_impurity = 0\n",
    "    best_feature_idx = 0\n",
    "    best_threshold = 0\n",
    "\n",
    "    # TODO - дополните реализацию функции получения наилучшего разделения\n",
    "\n",
    "    return best_gini_impurity, best_feature_idx, best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6WUknwBm584B"
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "X = np.array([1, 2, 3, 4, 5, 6, 7, 8]).reshape(-1, 1)\n",
    "y = np.array([1, 1, 1, 1, 0, 0, 0, 1])\n",
    "\n",
    "best_gini, best_feature_idx, best_threshold = get_best_split(X, y)\n",
    "\n",
    "assert np.isclose(best_gini, 0.1875)\n",
    "assert np.isclose(best_threshold, 4)\n",
    "assert best_feature_idx == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7cZglAoO6F-x"
   },
   "source": [
    "Проверим наши данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "executionInfo": {
     "elapsed": 2664,
     "status": "ok",
     "timestamp": 1601654556258,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "ajCAcUwrRuiP",
    "outputId": "c05769e5-522c-4e4a-dd80-337ecf2c0ef9"
   },
   "outputs": [],
   "source": [
    "best_gini, best_feature_idx, best_threshold = get_best_split(X_data, y_data)\n",
    "\n",
    "print(\n",
    "    f\"Best gini impurity:\\t{best_gini}\",\n",
    "    f\"\\nBest feature index:\\t{best_feature_idx}\",\n",
    "    f\"\\nBest threshold value:\\t{best_threshold}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TmdK4z86S8Pz"
   },
   "source": [
    "Теперь самое время реализовать второй вариант функции предсказания, которая будет производить предсказание на разделения по признаку и порогу, заданными через аргументы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ofC08EOdogN"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "def predict_v2(X, feature_index, threshold):\n",
    "    # Напишите реализацию функции предсказания\n",
    "    #   решающего дерева с одним узлом\n",
    "    #   разделение по признаку (x1) с порогом 0.5\n",
    "    # *Не забывайте о размерности данных X\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kYevB6eTTbqu"
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "X = X_data\n",
    "y_true = y_data\n",
    "\n",
    "y_pred = predict_v2(X, best_feature_idx, best_threshold)\n",
    "assert np.all(y_true == y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 613
    },
    "executionInfo": {
     "elapsed": 2958,
     "status": "ok",
     "timestamp": 1601654556584,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "w3PV6zIhTMme",
    "outputId": "60472378-96d2-4fee-866e-b59e9c3bbf59"
   },
   "outputs": [],
   "source": [
    "X = X_data\n",
    "y_true = y_data\n",
    "\n",
    "x1_vals = np.linspace(X[:,0].min()-0.5, X[:,0].max()+0.5, 100)\n",
    "x2_vals = np.linspace(X[:,1].min()-0.5, X[:,1].max()+0.5, 100)\n",
    "xx, yy = np.meshgrid(x1_vals, x2_vals)\n",
    "space_X = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "y_pred = predict_v2(space_X, best_feature_idx, best_threshold)\n",
    "y_pred = y_pred.reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx, yy, y_pred)\n",
    "pnts_scatter = plt.scatter(X[:, 0], X[:, 1], c=y_true, s=50, edgecolor='k')\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.grid(True)\n",
    "plt.legend(handles=pnts_scatter.legend_elements()[0], labels=['0', '1', '2'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLC01bPGUWdC"
   },
   "source": [
    "Как мы видим, лучшее разделение не произвело деление по 0.5, но тем не менее был найден такой признак и порог для него, который верно разделил все данные в наборе.\n",
    "\n",
    "Это отличный результат для автоматического построения решающего дерева, состоящего из одного корневого узла. Маленькими шагами мы идем от березки к огромным кедрам!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3dJ93ZIU4zS"
   },
   "source": [
    "# Представление структуры дерева"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FG6Etsu5U7zm"
   },
   "source": [
    "Один из насущных вопросов - как представить дерево в программе? Линейная и логистическая регрессия были формулами, по которым производился расчет, а здесь набор блоков \"если\" (узлов), причем у каждого должен быть задан индекс признака для разделения и порог."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HzGDNnqKVNlB"
   },
   "source": [
    "Но перво-наперво нам нужно взять задачку посложнее, так как эту мы уже решили!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 630
    },
    "executionInfo": {
     "elapsed": 3495,
     "status": "ok",
     "timestamp": 1601654557142,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "hiyWGDGaVT4i",
    "outputId": "6ea59429-9715-48aa-8324-147122a6d11c"
   },
   "outputs": [],
   "source": [
    "X_data, y_data = make_classification(\n",
    "    n_samples=100,\n",
    "    n_features=2, \n",
    "    n_redundant=0,\n",
    "    n_informative=2, \n",
    "    n_clusters_per_class=2,\n",
    "    random_state=3\n",
    ")\n",
    "\n",
    "pnts_scatter = plt.scatter(X_data[:, 0], X_data[:, 1], marker='o', c=y_data, s=50, edgecolor='k', )\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.grid(True)\n",
    "plt.legend(handles=pnts_scatter.legend_elements()[0], labels=['0', '1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nta8W8pKsVrk"
   },
   "source": [
    "Отлично, непростая задачка, но посмотрим, как справится полноценное решающее дерево!\n",
    "\n",
    "Давайте немного разберемся, как мы будем строить дерево. Здесь мы воспользуемся инструментом, которым можно как совершать великие дела, так и запросто выстрелись себе в ногу - **рекурсия**.\n",
    "\n",
    "> Если вы не помните, что такое рекурсия - обязательно обсудите с преподавателем! А ещё можете посмотреть [сюда](https://medium.com/nuances-of-programming/%D1%80%D0%B5%D0%BA%D1%83%D1%80%D1%81%D0%B8%D1%8F-%D0%B8-%D1%86%D0%B8%D0%BA%D0%BB-%D0%B2-%D1%87%D0%B5%D0%BC-%D1%80%D0%B0%D0%B7%D0%BD%D0%B8%D1%86%D0%B0-%D0%BD%D0%B0-%D0%BF%D1%80%D0%B8%D0%BC%D0%B5%D1%80%D0%B5-python-5f0064cc035f) \n",
    "\n",
    "![Решающее дерево](https://docs.google.com/uc?export=download&id=11RQsovSMaIgXnNDZrbytHC4xsoQi9Gxb)\n",
    "\n",
    "Смотрите, что мы будем делать. Мы имеем данные и нужно как-то построить дерево. Мы уже умеем выбирать разделение данных так, чтобы получить наибольший прирост Джини. Теперь нам нужно применить это знание, чтобы построить целое дерево!\n",
    "\n",
    "Начинаем мы с того, что создаем узел и передаем ему на обучение данные. Что может узел сделать с этими данными? Правильно, найти лучшее разделение! Полученный индекс признака и порог значения признака для разделения узел должен как-то запомнить, так как эти значения больше никак не изменятся. После этого те данные, которые были переданы для обучения мы делим с помощью найденных параметров (индекс признака и порог) на две \"кучки\" - та, что больше порога по этому признаку (правая), и та, что меньше (левая).\n",
    "\n",
    "Итого, узлу дали данные, а он нашел параметры разделения и поделил данные. Что делаем дальше? Создать два узла и в левый узел передать левую кучку данных для обучения а в правый - правую кучку.\n",
    "\n",
    "Если вы еще не заметили, где тут рекурсия, то вот она: самый первый узел в процессе обучения вызывает обучение дочерних (своих) узлов, а те вызывают обучение своих и так далее. То есть по сути, первый узел не закончит обучение, пока не закончат остальные...\n",
    "\n",
    "Но вот вопрос, а когда узлы закончат множиться? У дерева ведь должны быть листья, но мы их даже не обсуждали! Все верно, нам нужно определить условия, при которых нам нужно вместо нового узла создавать лист, который прервет рекурсию.\n",
    "\n",
    "Не будем таить и просто обсудим три основных условия, при которых нам нужно создавать узел:\n",
    "- Самое простое, если дерево слишком глубокое, то пора бы уже останавливаться; то есть зададимся максимальной глубиной и при построении нам нужно следить за тем, на какой глубине учится узел и, если, узел уже на максимальной глубине, то сразу делаем листья без всяких раздумий;\n",
    "- Еще одна несложная идея - задаемся минимальным количеством данных для обучения узла и, если после разделения внутри узла у нас в ветке (левой или правой) слишком мало данных, то делаем на этой ветке лист.\n",
    "- Последняя и самая простая - если так случилось, что после разделения в кучке (левой или правой) остались данные одного единственного класса, то смысл еще делить? Сразу делаем лист!\n",
    "\n",
    "Окей, вот мы создали лист и даже передали ему кучку данных после деления узла, что ему с ними делать? Учится лист очень просто - смотрим на данные и выясняем, записей какого класса в этих данных больше. Выяснили превалирующий класс - все, лист его запоминает и дальше он за этот класс и отвечает! Вот так просто!\n",
    "\n",
    "Итого:\n",
    "- Создаем первый узел;\n",
    "- В узле ищем лучший сплит и запоминаем параметры;\n",
    "- Делим данные на две кучки;\n",
    "- Проверяем условия для каждой кучки и решаем, слева и справа создаем новый узел или лист;\n",
    "- Вне зависимости от того, что создали передаем на обучение кучку (левому узлу/листу - левую кучку, с правой точно также);\n",
    "- Если узел, то все начинается с начала этого алгоритма;\n",
    "- Если лист, то выясняем мажорный класс в данных (данных которого больше в разметке), лист его запоминает и мы ничего нового не создаем.\n",
    "\n",
    "Как только все узлы понасоздают листы, то алгоритм завершится. Вот так несложно работает создание решающего дерева!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "levysv661GVV"
   },
   "source": [
    "Остался последний вопрос, а как предсказывать то? Вот дерево рекурсивно создано, теперь нам надо получить предсказания обученного дерева. Тут тоже несложно, давайте разберемся!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2U2LOH-Lc5gs"
   },
   "source": [
    "Если нам на предсказание поступает всего одна запись (одномерный вектор), то нет никаких проблем - мы его спускаем по узлам, пока не попадем на конкретный лист. Тот класс, который присвоен листу в результате обучения и будет предсказываемым классом!\n",
    "\n",
    "> Обратите внимание, во время предсказания ничего не строится!\n",
    "\n",
    "А что если на вход поступает набор новых данных (2D матрица)? Мы делаем похожим образом, как делали во время построения.\n",
    "\n",
    "![Решающее дерево](https://docs.google.com/uc?export=download&id=1cH-lEsm9L57sGyrU_3DNyyx4zaOvQIhF)\n",
    "\n",
    "На самый верхний (корневой) узел поступают данные. Этот узел имеет параметры разделения, поэтому узел делит данные на две группы и передает группы на ветви. Если данные (уже меньше, после разделения) попадают снова на узел, то делается то же самое. Снова рекурсия!\n",
    "\n",
    "По сути, проход по узлам только и делает, что делит данные и снова вызывает `predict()` левого и правого элемента.\n",
    "\n",
    "Как только какая-то часть данных попадает на лист, то он всем этим записям присваивает класс листа. Так информация о присвоении возвращается обратно, так как все равно все узлы так или иначе заканчиваются листами!\n",
    "\n",
    "Вот так несложно работает принцип прохода по решающему дереву для предсказания! Теперь пора написать наш код построения дерева и предсказания деревом!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3R68SGuYXdxn"
   },
   "source": [
    "Смотрите, нам нужно, чтобы узлы и листы хранили информацию внутри себя и удобно создавались, так как делать мы это будем часто! Прекрасно, классы и объекты - то, что нам нужно!\n",
    "\n",
    "Так как вся структура состоит из листов и узлов, то у нас и будет два класса: `DecisionLeaf` и `DecisionNode`. Между ними будет много общего, чтобы упростить написание кода. Такой подход называется **единообразным интерфейсом**, когда разные классы имеют одинаковые методы с одинаковыми аргументами. Это позволяет передавать данные на обучение или предсказание, не задумываясь о том, кому передаются данные!\n",
    "\n",
    "Сначала проработаем, какие общие черты будут у обоих классов:\n",
    "- принимать аргументом конструктора глубину, на которой они располагаются, чтобы затем узнать, какая глубина у всего дерева;\n",
    "- иметь методы для предсказания и обучения `.predict()` и `.fit()`;\n",
    "- иметь вспомогательные методы `.get_max_depth()`, который будет давать информацию о глубине, и `.print()`, который будет выводить информацию об элементе;\n",
    "\n",
    "Для начала выполним реализацию листа дерева. Метод для обучения листа `.fit()` будет заключаться в том, чтобы взять данные, которые приходят на этот лист для обучения и выбрать наиболее часто встречающийся класс, чтобы присвоить его листу. Так мы запомним, за какой класс отвечает этот лист!\n",
    "\n",
    "Суть метода предсказания `.predict()` проста - выдать класс листа, но только не просто число, а вектор с таким же размером (1D), сколько записей пришли на `predict()` листу. Мы ведь всем этим данным присваиваем класс листа! \n",
    "\n",
    "Метод получения информации о глубине `.get_max_depth()` просто вернет глубину листа, на котором он находится."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XqN-RA-diq-l"
   },
   "outputs": [],
   "source": [
    "class DecisionLeaf:\n",
    "    def __init__(self, depth):\n",
    "        ''' Конструктор класса\n",
    "        Аргументы\n",
    "        ---------\n",
    "            depth: int\n",
    "                глубина листа, на котором он располагается\n",
    "        '''\n",
    "        self.predict_class = None\n",
    "        self.depth = depth\n",
    "\n",
    "    def predict(self, X):\n",
    "        ''' Функция предсказания листа\n",
    "\n",
    "        Аргументы\n",
    "        ---------\n",
    "            X : ndarray [n_samples, n_features]\n",
    "                матрица данных\n",
    "\n",
    "        Возвращает\n",
    "        ----------\n",
    "            predict: ndarray [n_samples]\n",
    "                вектор предсказаний, заполненный значениями\n",
    "                класса листа\n",
    "        '''\n",
    "        # TODO - напишите функцию предсказания\n",
    "        return y_pred\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        ''' Метод находит в данных класс с наибольшим количеством записей\n",
    "            и присваивает его листу как наиболее вероятно \n",
    "            предсказываемый класс\n",
    "        \n",
    "        Аргументы\n",
    "        ---------\n",
    "            X : ndarray [n_samples, n_features]\n",
    "                матрица данных для обучения \n",
    "            y : ndarray [n_samples]\n",
    "                вектор истинных значений классов  \n",
    "        '''\n",
    "        # TODO - напишите функцию обучения\n",
    "        # Выбираем из y наиболее часто встречающееся значение \n",
    "        #   и присваиваем self.predict_class\n",
    "        #   это и будет предсказываемый класс листа\n",
    "\n",
    "    def get_max_depth(self):\n",
    "        ''' Получение информации о максимальной глубине\n",
    "        Возвращает\n",
    "        ----------\n",
    "            depth: int\n",
    "                глубина листа        \n",
    "        '''\n",
    "        # TODO - напишите функцию возврата глубины, на которой находится лист\n",
    "        return None\n",
    "\n",
    "    def print(self):\n",
    "        ''' Вывод информации о листе '''\n",
    "        print(f'{self.depth*\" \"}> Class {self.predict_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GmdgSViojJen"
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "leaf = DecisionLeaf(1)\n",
    "\n",
    "assert leaf.get_max_depth() == 1\n",
    "\n",
    "X = np.array([1, 1, 1, 3]).reshape(-1, 1)\n",
    "y = np.array([0, 1, 1, 2])\n",
    "leaf.fit(X, y)\n",
    "\n",
    "y_pred = leaf.predict(X)\n",
    "y_true = np.array([1, 1, 1, 1])\n",
    "\n",
    "assert np.all(y_pred == y_true)\n",
    "assert np.all(y_pred.shape == y_true.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pgOmmU2m73qe"
   },
   "source": [
    "Отлично, с листом разобрались, теперь реализация узла. \n",
    "\n",
    "Обучение узла `.fit()` будет делаться следующим образом:\n",
    "- получаем лучшее разделение данных, которые поступили на вход;\n",
    "- сохраняем в аттрибуты узла;\n",
    "- делаем сплит данных, которые были переданы для обучения;\n",
    "- проверяем условия создания листа;\n",
    "    - если надо создать лист - в `self.true_elem` создаем лист;\n",
    "    - иначе создаем в `self.true_elem` узел;\n",
    "- передаем в правый элемент левую часть данных (после сплита) в метод `.fit()`;\n",
    "- то же самое с левой частью данных (`self.false_elem`);\n",
    "\n",
    "После построения надо сделать реализацию метода предсказания. Метод предсказания `.predict()` будет заключаться в том, чтобы разделить поступившие для предсказания данные и передать части в соответсвующие ветки дальше для `.predict()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vP8IKh0ml2ij"
   },
   "outputs": [],
   "source": [
    "class DecisionNode:\n",
    "    def __init__(self, depth, depth_limit, min_samples_split):\n",
    "        ''' Конструктор класса\n",
    "        Аргументы\n",
    "        ---------\n",
    "            depth: int\n",
    "                глубина узла, на которой он располагается\n",
    "\n",
    "            depth_limit: int\n",
    "                максимальная глубина дерева\n",
    "            \n",
    "            min_samples_split: int\n",
    "                минимальное количество записей для создания узла\n",
    "        '''\n",
    "        # Глубина, на которой узел находится\n",
    "        self.depth = depth\n",
    "        # Максимальная глубина\n",
    "        self.depth_limit = depth_limit\n",
    "        # Минимальное количество записей после сплита, чтобы создать узел\n",
    "        self.min_samples_split = min_samples_split\n",
    "        # Индекс признака, по которому узел делает разделение\n",
    "        self.feature_index = None\n",
    "        # Порог для разделения\n",
    "        self.threshold = None\n",
    "\n",
    "        # Аттрибуты для веток (правая ~ true, левая ~ false)\n",
    "        self.true_elem = None\n",
    "        self.false_elem = None\n",
    "    \n",
    "    def _create_new_element(self, X, y):\n",
    "        ''' Метод создания нового элемента\n",
    "        \n",
    "        Аргументы\n",
    "        ---------\n",
    "            X : ndarray [n_samples, n_features]\n",
    "                матрица данных для обучения \n",
    "            y : ndarray [n_samples]\n",
    "                вектор истинных значений классов  \n",
    "        '''\n",
    "        # Если в разметке остались уникальные классы - создаем лист\n",
    "        if len(set(y)) == 1:\n",
    "            return DecisionLeaf(self.depth+1)\n",
    "        # TODO - допишите ограничения \n",
    "        #   на минимальное количество записей в данных\n",
    "        #   и ограничение глубины\n",
    "\n",
    "\n",
    "        # Если так и не вернули лист - то возвращаем узел\n",
    "        # У него увеличиваем глубину на 1 и пробрасываем инфу об ограничениях\n",
    "        return DecisionNode(\n",
    "            self.depth+1, \n",
    "            self.depth_limit,\n",
    "            self.min_samples_split\n",
    "        )\n",
    "\n",
    "    def predict(self, X):\n",
    "        ''' Функция предсказания узла\n",
    "\n",
    "        Аргументы\n",
    "        ---------\n",
    "            X : ndarray [n_samples, n_features]\n",
    "                матрица данных\n",
    "\n",
    "        Возвращает\n",
    "        ----------\n",
    "            predict: ndarray [n_samples]\n",
    "                вектор предсказаний\n",
    "        '''\n",
    "        # TODO - напишите реализацию метода предсказания\n",
    "\n",
    "        # Получите маску разделения\n",
    "\n",
    "        # Вот формируем вектор предсказания\n",
    "        prediction = np.ndarray(X.shape[0], dtype=int)\n",
    "\n",
    "        # Вот заполняем предсказания одной ветви\n",
    "        prediction[~mask] = self.false_elem.predict(left_X)\n",
    "        \n",
    "        # Сделайте заполнения для второй ветви\n",
    "\n",
    "        return prediction\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        ''' Метод обучения узла\n",
    "        \n",
    "        Аргументы\n",
    "        ---------\n",
    "            X : ndarray [n_samples, n_features]\n",
    "                матрица данных для обучения \n",
    "            y : ndarray [n_samples]\n",
    "                вектор истинных значений классов  \n",
    "        '''\n",
    "        # TODO - напишите реализацию метода обучения\n",
    "\n",
    "        # Получите лучший сплит\n",
    "\n",
    "        # Сохраните параметры сплита в self.feature_index и self.threshold\n",
    "\n",
    "        # Вот здесь мы создаем маску для деления\n",
    "        mask = X[:, self.feature_index] > self.threshold\n",
    "        right_X = X[mask]\n",
    "        right_y = y[mask]\n",
    "\n",
    "        self.true_elem = self._create_new_element(right_X, right_y)\n",
    "        self.true_elem.fit(right_X, right_y)\n",
    "        \n",
    "        # Вам нужно сделать аналогичные действия для другой ветки\n",
    "\n",
    "\n",
    "    def get_max_depth(self):\n",
    "        ''' Получение информации о максимальной глубине\n",
    "        Возвращает\n",
    "        ----------\n",
    "            depth: int\n",
    "                глубина листа        \n",
    "        '''\n",
    "        # Берем максимум от максимальной глубины по веткам\n",
    "        return max([\n",
    "            self.true_elem.get_max_depth(), \n",
    "            self.false_elem.get_max_depth()\n",
    "        ])\n",
    "    \n",
    "    def print(self):\n",
    "        ''' Вывод информации об узле '''\n",
    "        print(f'{self.depth*\" \"}| {self.feature_index} > {self.threshold}')\n",
    "        self.true_elem.print()\n",
    "        self.false_elem.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lLVpTPWtq6Q6"
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "# Допустим, что узел на уровне 1 и максимальная глубина = 2\n",
    "# Ограничение на минимальное количество уберем\n",
    "node = DecisionNode(1, 2, 0)\n",
    "\n",
    "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
    "y = np.array([0, 0, 1, 1, 0])\n",
    "node.fit(X, y)\n",
    "\n",
    "y_pred = node.predict(X)\n",
    "y_true = np.array([0, 0, 1, 1, 1])\n",
    "\n",
    "assert node.get_max_depth() == 2\n",
    "assert np.all(y_pred == y_true)\n",
    "assert np.all(y_pred.shape == y_true.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3YwilQ9LMMCc"
   },
   "source": [
    "Уф, немаленький класс получился! Осталось написать класс, который будет зваться `DecisionTree` и являться по сути нашей моделью!\n",
    "\n",
    "> На самом деле могли бы прямо так использовать класс `DecisionNode`, но мы создаем класс-обертку, как для того, чтобы было понятно по названию, так и для того, чтобы можно было в будущем какие-то отдельные вещи в нем реализовывать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0AqV6kesarIL"
   },
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, depth_limit, min_samples_split):\n",
    "        ''' Конструктор класса\n",
    "        \n",
    "        Аргументы\n",
    "        ---------\n",
    "            depth_limit: int\n",
    "                максимальная глубина дерева\n",
    "            \n",
    "            min_samples_split: int\n",
    "                минимальное количество записей для создания узла\n",
    "        '''\n",
    "        self.root = DecisionNode(0, depth_limit, min_samples_split)\n",
    "\n",
    "    def predict(self, X):\n",
    "        ''' Функция предсказания узла\n",
    "\n",
    "        Аргументы\n",
    "        ---------\n",
    "            X : ndarray [n_samples, n_features]\n",
    "                матрица данных\n",
    "\n",
    "        Возвращает\n",
    "        ----------\n",
    "            predict: ndarray [n_samples]\n",
    "                вектор предсказаний\n",
    "        '''\n",
    "        return self.root.predict(X)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        ''' Функция обучения\n",
    "\n",
    "        Аргументы\n",
    "        ---------\n",
    "            X : ndarray [n_samples, n_features]\n",
    "                матрица данных\n",
    "        '''\n",
    "        self.root.fit(X, y)\n",
    "\n",
    "    def get_depth(self):\n",
    "        ''' Получение информации о глубине дерева\n",
    "\n",
    "        Возвращает\n",
    "        ----------\n",
    "            depth: int\n",
    "                глубина листа        \n",
    "        '''\n",
    "        return self.root.get_max_depth()\n",
    "    \n",
    "    def print(self):\n",
    "        ''' Вывод информации о дереве '''\n",
    "        self.root.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E2KJMEy4elWD",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "X = X_data\n",
    "y_true = y_data\n",
    "\n",
    "# Снимем ограничения дерева\n",
    "# Не ограничиваем глубину и минимальное кол-во записей для узла\n",
    "tree = DecisionTree(100, 0)\n",
    "tree.fit(X,y_true)\n",
    "\n",
    "assert tree.get_depth() == 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 586
    },
    "executionInfo": {
     "elapsed": 3788,
     "status": "ok",
     "timestamp": 1601654557485,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "0-AHOdNHMs07",
    "outputId": "979fa1a7-bec1-4c19-ab84-94aeb44ae310"
   },
   "outputs": [],
   "source": [
    "tree.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qLdy0_16M0b2"
   },
   "source": [
    "Если дерево обучилось, тест на соответствующую глубину пройден - можно взглянуть на пространство принятия решений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eYolURdmfHML"
   },
   "outputs": [],
   "source": [
    "def plot_tree_decision_space(X, y_true, tree):\n",
    "    x1_vals = np.linspace(X[:,0].min()-0.5, X[:,0].max()+0.5, 300)\n",
    "    x2_vals = np.linspace(X[:,1].min()-0.5, X[:,1].max()+0.5, 300)\n",
    "    xx, yy = np.meshgrid(x1_vals, x2_vals)\n",
    "    space_X = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "    y_pred = tree.predict(space_X)\n",
    "    y_pred = y_pred.reshape(xx.shape)\n",
    "\n",
    "    plt.contourf(xx, yy, y_pred)\n",
    "    pnts_scatter = plt.scatter(X[:, 0], X[:, 1], c=y_true, s=50, edgecolor='k')\n",
    "    plt.xlabel(\"$x_1$\")\n",
    "    plt.ylabel(\"$x_2$\")\n",
    "    plt.grid(True)\n",
    "    plt.legend(handles=pnts_scatter.legend_elements()[0], labels=['0', '1', '2'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 613
    },
    "executionInfo": {
     "elapsed": 4152,
     "status": "ok",
     "timestamp": 1601654557872,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "kliQINdhTtin",
    "outputId": "a1c57350-46f5-473d-af5a-0da86c30b6dd"
   },
   "outputs": [],
   "source": [
    "plot_tree_decision_space(X, y_true, tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJPTBggZNVeq"
   },
   "source": [
    "Обратите внимание, как нелинейно произошло разделение пространства! Давайте для простой проверки посмотрим, сколько элементов не соответсвует вектору истинных значений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "executionInfo": {
     "elapsed": 4132,
     "status": "ok",
     "timestamp": 1601654557873,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "axxVL5sYgjpj",
    "outputId": "8bf516d2-36a8-4018-d023-0d282012989c"
   },
   "outputs": [],
   "source": [
    "y_pred = tree.predict(X)\n",
    "(y_pred != y_true).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DMPLmLMNwa3"
   },
   "source": [
    "Вот это да! Вектор предсказаний полностью соответствует вектору\n",
    "истинных значений! В таких ситуациях важно не поддаться избыточной радости - важно всячески проверить, насколько хорошо работает модель."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tMUcDIcBOAEF"
   },
   "source": [
    "## Задание\n",
    "\n",
    "Произведите разделение на обучение/тест выборки, обучите дерево и отобразите матрицу ошибок."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2537no0dOL9k"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vmks0GZyOQAL"
   },
   "source": [
    "По результатам проверки мы увидели, что наше дерево переобучилось. Почему так случилось? Потому что это одна из основных особенностей решающих деревьев - **без каких-либо ограничений дерево точно переобучится**. Ведь дерево может бесконечно долго учиться, пока не разделит данные идеально."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kdSifsdEURUe"
   },
   "source": [
    "## Задание\n",
    "\n",
    "Определите наилучшие показатели для дерева с ограничением по показателю f1 кроссвалидацией путем перебора двух параметров. Постройте таблицу и отобразите ее с помощью `seaborn.heatmap()`.\n",
    "\n",
    "> Для вычисления показателя воспользуйтесь функцией из модуля `sklearn.metrics.f1_score` https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "\n",
    "> Для получения К-фолд со стратификацией воспользуйтесь функцией из модуля `sklearn.model_selection.StratifiedKFold` https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Spxl64YLuxE4"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "\n",
    "def cross_val_score_f1(model, k_folds, X, y):\n",
    "    f1_values = []\n",
    "    # TODO - напишите функцию получения оценки кросс-валидацией\n",
    "    \n",
    "    return np.mean(f1_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eou67ALr6Rt2"
   },
   "outputs": [],
   "source": [
    "X = X_data\n",
    "y_true = y_data\n",
    "\n",
    "for depth_limit in range(1, 7):\n",
    "    for min_samples in range(0, 20, 2):\n",
    "        tree = DecisionTree(depth_limit=depth_limit, min_samples_split=min_samples)\n",
    "        f1_value = cross_val_score_f1(tree, 5, X, y_true)\n",
    "\n",
    "# TODO - Добавьте сохранение данных и отображение таблицы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VxfOE9GLX1Rq"
   },
   "source": [
    "После того, как мы успешно вырастили дерево самое время вырастить лес из таких деревьев! Но перед этим нам надо познакомиться с такими понятиями как **ансамблирование** и **бэггинг**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "66UUiM4qYn6c"
   },
   "source": [
    "# Ансамблирование (Ensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30pVuMHHYpns"
   },
   "source": [
    "Всем известно определение слова ансамбль (из большой советской энциклопедии):\n",
    "> Ансамбль I Анса́мбль (франц. ensemble, буквально — вместе, сразу)\n",
    "совокупность, стройное целое.\n",
    "\n",
    "Идея ансамблирования в машинном обучении заключается в том, что какой бы супер-классификатор мы не сделали, много маленьких слабых классификаторов, работающих вместе, будут выполнять задачу лучше.\n",
    "\n",
    "Как мы ранее видели, такой классификатор, как решающее дерево, при сильном ограничении не позволяет достаточно разделить пространство, чтобы отразить зависимость в данных, а при слабом ограничении растет и запоминает данные, что приводит к переобучению. Группа таких классификаторов же может дать хороший результат, даже если каждый из них будет совершать ошибки, то усреднение результатов может дать более точный ответ.\n",
    "\n",
    "Для организации обучения используется второй термин, который мы рассмотрим.\n",
    "\n",
    "> Еще, можете почитать про историю с быком и что такое \"Мудрость толпы\": https://econs.online/articles/ekonomika/sila-i-slabost-kollektivnogo-razuma/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7_se55yrYDv-"
   },
   "source": [
    "# Бэггинг (bagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rruCarg7YSNu"
   },
   "source": [
    "Термин является сокращением от **b**ootstrap **agg**regat**ing**. Методика бутстрэпинга (bootstraping) известна в статистике и заключается в том, что для усредненной оценки показателей все данные делятся на группы. Эти группы могут пересекаться, то есть, например, у нас есть массив чисел:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0 & 1 & 2 & 3 & 4\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Мы создаем две выборки по четыре элемента в каждой, тогда метод бутстрэпинга может дать результат\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0 & 3 & 3 & 1\n",
    "\\end{bmatrix}\n",
    "и\n",
    "\\begin{bmatrix}\n",
    "1 & 4 & 2 & 4\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "> Сам по себе метод бутстрэпинга - это метод выборки, при котором мы берем случайную (по равномерному распределению) запись из данных, заносим ее в новую выборку, но не исключаем из исходных данных. Тогда эта же запись может попасть в ту же выборку еще раз.\n",
    "\n",
    "> Можно проверить, если взять исходные данные вычислить среднее, а затем создать несколько бутстреп выборок и получить усредненное из средних значений каждой выборки. Исходное среднее и усредненное значение средних должны быть приблизительно равны.\n",
    "\n",
    "Так при чем тут бэггинг?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lxr2lrCecquc"
   },
   "source": [
    "Бэггинг, являясь одним из простых методов ансамблирования, берет идею таких выборок: положим, что имеются данные с $N$ записями, тогда создадим $D$ бутстрэп выборок (каждая размером $N$) и обучим $D$ моделей, каждая на своем кусочке. В чем преимущество такого подхода?\n",
    "\n",
    "> Модели не видят всех записей данных, так что переобучение каждой модели на своем кусочке данных не так страшно - так как результаты модели усредняются, то переобученность каждой из моделей на части данных не сказывается на конечном предсказании."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gj0lNEqwkLpy"
   },
   "source": [
    "Для практики реализуем метод генерации бутстрэп выборок из исходных данных: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iSHp3AJivrIq"
   },
   "outputs": [],
   "source": [
    "def generate_bootstrap_sets(X, D):\n",
    "    ''' Генерация D выборок из исходных данных\n",
    "\n",
    "    Параметры\n",
    "    ---------\n",
    "    X: [n_samples, n_features]\n",
    "        Исходные данные\n",
    "    \n",
    "    D: int\n",
    "        Количество выборок для генерации\n",
    "\n",
    "    Возвращает\n",
    "    ----------\n",
    "    boostrapped_sets: list\n",
    "        Список, состоящий из бутстрэп-выборок данных размером [n_samples, n_features]\n",
    "    ''' \n",
    "\n",
    "    bootstrapped_sets = []\n",
    "    # TODO - напишите реализацию генератора выборок\n",
    "    return bootstrapped_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inBrYdQflXQB"
   },
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    [1, 2, 3],\n",
    "    [5, 7, 9],\n",
    "    [-2, 0, 3],\n",
    "    [-1, 9, 2],\n",
    "])\n",
    "\n",
    "boostrapped_Xs = generate_bootstrap_sets(X, 3)\n",
    "\n",
    "assert len(boostrapped_Xs) == 3\n",
    "assert boostrapped_Xs[0].shape == X.shape\n",
    "assert boostrapped_Xs[1].shape == X.shape\n",
    "assert boostrapped_Xs[2].shape == X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ukAvIFSQoGO3"
   },
   "source": [
    "# Бэггинг над решающими деревьями"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kCizuWWKoKbS"
   },
   "source": [
    "После реализации функции генерации бутстрэп выборок мы можем проверить работоспособность подхода ансемблирования, действительно ли он работает лучше, чем одно решающее дерево.\n",
    "\n",
    "Реализацию построим по тому же принципу на основе классов. Создадим класс `BaggingDecisionTrees`, который будет также иметь методы `.predict()` и `.fit()`. В качестве аргументов конструктора будем задавать количество деревьев для генерации и максимальную глубину деревьев."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WMM53FSMv9fL"
   },
   "outputs": [],
   "source": [
    "class BaggingDecisionTrees:\n",
    "    def __init__(self, ensemble_size, depth_limit, min_samples_split):\n",
    "        self.ensemble_size = ensemble_size\n",
    "        self.depth_limit = depth_limit\n",
    "        self.min_samples_split = min_samples_split\n",
    "    \n",
    "        # TODO - Напишите код создания необходимого количества решающих деревьев\n",
    "        self.trees = []\n",
    "\n",
    "    def predict(self, X):\n",
    "        predicts = []\n",
    "        result_predictions = np.ndarray(X.shape[0], dtype=int)\n",
    "\n",
    "        # TODO - Получите предсказания каждого дерева в массив predicts\n",
    "        \n",
    "        # А вот так мы получим голосование по каждой записи\n",
    "        # Так как у нас predict - это массив (n_samples, ensemble_size),\n",
    "        #   то по каждой записи проводим голосование \n",
    "        #   и выбираем наиболее частый класс среди предсказаний моделей\n",
    "        predicts = np.array(predicts).T\n",
    "        for i_s in range(predicts.shape[0]):\n",
    "            (uniques, counts) = np.unique(predicts[i_s], return_counts=True)\n",
    "            most_frequent = uniques[np.argmax(counts)]\n",
    "            # TODO - осталось заполнить правильное значение \n",
    "            #   в векторе предсказаний\n",
    "\n",
    "        return result_predictions\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Так мы объединили данные для генерации выборок\n",
    "        Xy = np.c_[X, y]\n",
    "        # TODO - добавьте здесь генерацию выборов\n",
    "        for i, Xy in enumerate(boostrapped_Xy):\n",
    "            X = Xy[:, :-1]\n",
    "            y = Xy[:, -1]\n",
    "            # TODO - здесь добавьте обучение i-го дерева\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2R0g-0cGrFZX"
   },
   "outputs": [],
   "source": [
    "X = X_data\n",
    "y_true = y_data\n",
    "\n",
    "bag_tree = BaggingDecisionTrees(\n",
    "    ensemble_size=10, \n",
    "    depth_limit=3, \n",
    "    min_samples_split=2\n",
    ")\n",
    "bag_tree.fit(X, y_true)\n",
    "y_pred = bag_tree.predict(X)\n",
    "\n",
    "assert len(bag_tree.trees) == 10\n",
    "assert np.all(y_pred.shape == y_true.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 613
    },
    "executionInfo": {
     "elapsed": 41006,
     "status": "ok",
     "timestamp": 1601654594804,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "Loc61lWyrjR_",
    "outputId": "d89e388a-7e57-4309-c945-f57b7311aba0"
   },
   "outputs": [],
   "source": [
    "plot_tree_decision_space(X, y_true, bag_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ai3rGEwC8sAI"
   },
   "source": [
    "## Задание\n",
    "\n",
    "Определите наилучшие параметры для бэггинга над решающими деревьями путем перебора трех параметров. Для каждого параметра зафиксируйте список проверяемых значений и в трех циклах перебором подставляйте значения и определите лучшие для данной модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EW9VeI_19FsR"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K-pES6ht9F9B"
   },
   "source": [
    "# Поиск гиперпараметров методом сетки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64S2S4Bw9J4S"
   },
   "source": [
    "> **Гиперпараметры** - параметры, относящиеся к структуре или метаинформации модели. Так называются, потому что термин **параметры** относится к обучаемым сущностям в модели (веса, узлы и т.д.). Гиперпараметры в свою очередь не обучаются, а устанавливают то, насколько модель велика, коэффициенты регуляризации и др.\n",
    "\n",
    "Методы поиска гиперпараметров различны, но мы рассмотрим один из наиболее распространенных - метод поиска сеткой.\n",
    "\n",
    "На самом деле, мы уже его рассмотрели и даже практиковались - в прошлом задании! Суть метода заключается в том, что для каждого гиперпараметра фиксируется набор значений и далее перебором всех возможных комбинаций кросс-валидацией производится поиск лучшего набора параметров.\n",
    "\n",
    "Вот так, вроде умное название, а метод очень простой!\n",
    "\n",
    "Другой известный метод - метод случайного поиска. В данном методе комбинации создаются не за счет заданных значений гиперпараметров, а заданием диапазона значений и уже далее для каждой комбинации конкретное значений получают за счет случайного выбора из этого диапазона.\n",
    "\n",
    "В качестве практики попробуйте реализовать метод случайного поиска гиперпараметров:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WlQWPz6x-hgZ"
   },
   "outputs": [],
   "source": [
    "# TODO - реализуйте поиск лучшего варианта бэггинга над решающими деревьями методом случаного выбора гиперпараметров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wG515v1A-h7h"
   },
   "source": [
    "# Случайный лес"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ohiv4abQ-kcZ"
   },
   "source": [
    "Наконец, перейдем к понятию случайного леса, ведь кажется, ансамбль деревьев и есть случайный лес, разве нет? Ответ: не совсем. В ходе развития метод бэггинга стал основополагающим в построении работы случайного леса, но существует очень важное отличие!\n",
    "\n",
    "Заключается оно в методе разделения узла в дереве. До этого для получения наилучшего сплита мы рассматривали все $N$ записей и все $M$ признаков, чтобы найти наилучшее разделение. Случайный лес вводит маленькое ограничение: для получения сплита рассматриваются не все признаки из данных, а лишь $F (< M)$ случайных признаков.\n",
    "\n",
    "> Для задач классификации рекомендуется брать $F = \\sqrt{M}$ признаков. Мы не рассматриваем деревья для регрессии, но там такие же принципы и там рекомендуется брать $F = M/3$.\n",
    "\n",
    "Теперь, для реализации модифицируем класс `DecisionNode`, добавив новый гиперпараметр. Это требуется сделать как в конструкторе, так и в логике обучения. Метод обучения будет изменен таким образом, что функции поиска лучшего сплита будут переданы не все колонки из данных, а лишь указанное в гиперпараметре количество. Выбираться колонки будут случайно.\n",
    "\n",
    "Следующее задание на написание кода будет нестандартным. Обычно так делать не приходится, но из-за системы в Jupyter нам нужно в следующую кодовую ячейку вставить старую реализацию `DecisionNode` и модифицировать ее:\n",
    "- добавить в конструктор аргумент `max_features` и сохранить его в атрибутах объекта (`self.max_features = max_features`);\n",
    "- при создании нового узла ему нужно передавать значение `self.max_features`;\n",
    "- в методе `.fit()` добавьте формирование новой матрицы данных, в которых случано выбраны `max_feature` признаков (колонок) - может помочь `np.random.choice`; не забывайте, что `get_best_split()` выдает индекс признака по той матрице, которая была передана, а мы ее только что покрамсали по колонкам; \n",
    "\n",
    "<details>\n",
    "<summary>Подсказка по последней части</summary>\n",
    "\n",
    "```Python\n",
    "        split_feature_indices = np.random.choice(range(X.shape[1]), self.max_features, replace=False)\n",
    "        _, best_feature_idx, best_threshold = get_best_split(X[:, split_feature_indices], y)\n",
    "\n",
    "        # Не забудьте, что get_best_split не знает об истинных индексах\n",
    "        self.feature_index = split_feature_indices[best_feature_idx]\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Px9XQN_v2_bE"
   },
   "outputs": [],
   "source": [
    "# TODO - сделайте новую реализацию в этой ячейке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vReB4aog31vN"
   },
   "source": [
    "Теперь обновим класс `DecisionTree`, добавив новый аргумент:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QS6_vsdyC0E2"
   },
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, depth_limit, min_samples_split, max_features):\n",
    "        ''' Конструктор класса\n",
    "        Аргументы\n",
    "        ---------\n",
    "            depth_limit: int\n",
    "                максимальная глубина дерева\n",
    "            \n",
    "            min_samples_split: int\n",
    "                минимальное количество записей для создания узла\n",
    "\n",
    "            max_features: int\n",
    "                максимальное количество признаков для деления\n",
    "        '''\n",
    "        self.root = DecisionNode(0, depth_limit, min_samples_split, max_features)\n",
    "\n",
    "    def predict(self, X):\n",
    "        ''' Функция предсказания узла\n",
    "\n",
    "        Аргументы\n",
    "        ---------\n",
    "            X : ndarray [n_samples, n_features]\n",
    "                матрица данных\n",
    "\n",
    "        Возвращает\n",
    "        ----------\n",
    "            predict: ndarray [n_samples]\n",
    "                вектор предсказаний\n",
    "        '''\n",
    "        return self.root.predict(X).astype(int)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        ''' Функция предсказания\n",
    "\n",
    "        Аргументы\n",
    "        ---------\n",
    "            X : ndarray [n_samples, n_features]\n",
    "                матрица данных\n",
    "\n",
    "        Возвращает\n",
    "        ----------\n",
    "            predict: ndarray [n_samples]\n",
    "                вектор предсказаний\n",
    "        '''\n",
    "        self.root.fit(X, y)\n",
    "\n",
    "    def get_depth(self):\n",
    "        ''' Получение информации о глубине дерева\n",
    "        Возвращает\n",
    "        ----------\n",
    "            depth: int\n",
    "                глубина листа        \n",
    "        '''\n",
    "        return self.root.get_max_depth()\n",
    "    \n",
    "    def print(self):\n",
    "        ''' Вывод информации о дереве '''\n",
    "        self.root.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pD2PnZHx39V8"
   },
   "source": [
    "И напишем реализацию случайного леса!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zv7gsIXkC56d"
   },
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "    def __init__(self, ensemble_size, depth_limit, min_samples_split, max_features):\n",
    "        self.ensemble_size = ensemble_size\n",
    "        self.trees = []\n",
    "        # TODO - Напишите код создания необходимого количества решающих деревьев\n",
    "\n",
    "    def predict(self, X):\n",
    "        # TODO - напишите метод предсказания\n",
    "\n",
    "        return result_predictions\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        pass\n",
    "        # TODO - напишите метод предсказания"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q1koJXagDNQi"
   },
   "source": [
    "Самое время проверить разработанный случайный лес обучением:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7wFJQ7dxDM3d"
   },
   "outputs": [],
   "source": [
    "X = X_data\n",
    "y_true = y_data\n",
    "\n",
    "rf_clfr = RandomForest(\n",
    "    ensemble_size=10, \n",
    "    depth_limit=3, \n",
    "    min_samples_split=2,\n",
    "    max_features=int(np.sqrt(X.shape[1]))\n",
    ")\n",
    "rf_clfr.fit(X, y_true)\n",
    "\n",
    "y_pred = rf_clfr.predict(X)\n",
    "\n",
    "assert len(rf_clfr.trees) == 10\n",
    "assert np.all(y_pred.shape == y_true.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 613
    },
    "executionInfo": {
     "elapsed": 44938,
     "status": "ok",
     "timestamp": 1601654598796,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "LS_ujM7ADuBn",
    "outputId": "a906b578-b8e3-4679-e4aa-54fdc6fb6515"
   },
   "outputs": [],
   "source": [
    "plot_tree_decision_space(X, y_true, rf_clfr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YK7gVTWqcvFJ"
   },
   "source": [
    "## Задание\n",
    "\n",
    "Найдите лучшие параметры для случайного леса и сравните с лучшим вариантом модели бэггинга над решающими деревьями. Параметр `max_features` менять не требуется. Для оценки как всегда используйте кросс-валидацию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JGh4GlME8Lxt"
   },
   "source": [
    "# Лучшая стандартизация! - Задание\n",
    "\n",
    "Кто-то может сказать, что все рассматриваемые данные до этого были слишком простыми - а как вам такие? Решаем новую задачку!\n",
    "\n",
    "Попробуйте найти лучшие параметры для моделей решающего дерева и случайного леса для следующих данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 560
    },
    "executionInfo": {
     "elapsed": 44916,
     "status": "ok",
     "timestamp": 1601654598797,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "aQUr2cX18gGk",
    "outputId": "323e3bae-c155-44e5-cbf2-2c4c0f55e7e4"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X_data, y_data = make_moons(\n",
    "    n_samples=1000,\n",
    "    noise=.1,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "X_data[:,0] *= 1000\n",
    "\n",
    "plt.figure(figsize=[15, 9])\n",
    "pnts_scatter = plt.scatter(X_data[:, 0], X_data[:, 1], marker='o', c=y_data, s=50, edgecolor='k')\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4fuZvvec8v9Z"
   },
   "source": [
    "Разработайте лучшие модели, после этого проведите стандартизацию данных и оцените, как она влияет на модели, использующие решающие деревья. То есть сравните работу лучших моделей со стандартизацией и без.\n",
    "\n",
    "> Если помните, для моделей линейной и логистической регрессий стандартизация играла большую роль! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6CLPrpJ_9IbJ"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-F-ey3yCwsg"
   },
   "source": [
    "# Регрессия для вас - Задание\n",
    "\n",
    "Мы не рассматривали случай, когда лес (а соответственно и деревья) используется для решения задачи регрессии. Вы уже знаете основные принципы построения деревьев для решения задачи классификации - попробуйте решить задачу регрессии с помощью дерева (а дальше несложно применить и лес). \n",
    "\n",
    "Вот вам данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "executionInfo": {
     "elapsed": 1059,
     "status": "ok",
     "timestamp": 1601654599884,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "SR2NC0_KDGrj",
    "outputId": "66b65b45-5a45-434b-99ad-b977ea1048f6"
   },
   "outputs": [],
   "source": [
    "X_data = np.linspace(-1, 7, 200)[:, None]\n",
    "y_data = np.sin(X_data[:,0])*5 + np.random.normal(size=X_data.shape[0])*2 + 5\n",
    "\n",
    "# Посмотрим на данные\n",
    "plt.scatter(X_data[:,0], y_data)\n",
    "plt.grid(True)\n",
    "plt.xlabel('Значение признака ($x$)')\n",
    "plt.ylabel('Истинное значение ($y$)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FoSu5oq1DRwB"
   },
   "source": [
    "Суть задания в том, чтобы самостоятельно написать реализацию класса дерева `DecisionTreeReg` (узла `DecisionLeafReg` и листа `DecisionNodeReg`, соответственно) для задачи регрессии. Отличий очень мало, давайте их рассмотрим:\n",
    "- Вместо примесей Джини используется уже знакомый MSE;\n",
    "- В ходе обучения условия создания листа - \"максимальная глубина\", \"минимальное количество данных для нового узла\" остаются, проверку на уникальность оставшейся разметки можно убрать;\n",
    "- Поиск наилучшего сплита раньше делался так:\n",
    "    - Вычисляем примеси Джини на наборе данных;\n",
    "    - Берем запись в данных (точку), берем ее первый признак, берем значение этого признака в качестве порога;\n",
    "    - Делаем сплит по этим параметрам (индекс признака, значение порога)\n",
    "    - Вычисляем примеси на левой и правой кучках;\n",
    "    - Берем взвешенную сумму (веса = количество в кучке после сплита / все данные) левой и правой кучек показателей примесей;\n",
    "    - Проверяем, насколько снизился показатель примесей;\n",
    "    - Лучший сплит тот, который уменьшил сильнее всего примеси!  \n",
    "- Теперь мы делаем чуть-чуть по-другому:\n",
    "    - Вычисляем MSE на наборе данных (предсказание - это среднее значение всего `y`, так что каждую точку отнимаем от среднего по `y`);\n",
    "    > Ну почти MSE, можно назвать СКО (STD) - средне квадратичное отклонение =)\n",
    "    - Берем запись в данных (точку), берем ее первый признак, берем значение этого признака в качестве порога;\n",
    "    - Делаем сплит по этим параметрам (индекс признака, значение порога)\n",
    "    - Вычисляем MSE (или STD, кто как назовет) на левой и правой кучках; то есть в левой берем среднее и от него отнимаем каждую точку левой кучки, аналогично справа;\n",
    "    - Берем взвешенную сумму (веса = количество в кучке после сплита / все данные) левой и правой MSE;\n",
    "    - Проверяем, насколько снизился показатель MSE;\n",
    "    - Лучший сплит тот, который уменьшил сильнее всего показатель!\n",
    "- Обучение листа - теперь не большинство в разметке в `y`, а среднее значение `y`;\n",
    "- Предсказания, у узла то же самое, просто делим данные и доносим до листьев, а вот лист просто предсказывает то среднее, что он запомнил во время обучения;\n",
    "- `max_features` для задачи регрессии лучше брать не $\\sqrt{M}$, а $M/3$;\n",
    "\n",
    "По сути, самое большое - поменять алгоритм сплита, в остальном почти также)\n",
    "А что поменяется в лесу? Теперь предсказания 10 деревьев не голосованием выбираются, а просто среднее по всем.\n",
    "\n",
    "Все достаточно просто, так что верим в себя и дерзайте!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BHPwVS63KQT6"
   },
   "outputs": [],
   "source": [
    "# TODO - дерево для регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lISHdEfzIwl3"
   },
   "source": [
    "# Выводы - Задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mLovZOY-Bm61"
   },
   "source": [
    "Напишите выводы по итогам изучения материала. Ниже найдёте вспомогательные вопросы, которые не должны сковывать ваше мышление. Собственное мнение и умозаключения - приветствуются!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WuKPFu1qIyfP"
   },
   "source": [
    "1. Что делает дерево, может ли оно заменить задачу регресии? \n",
    "2. Зачем нужно понимать глубину дерева? \n",
    "3. Как производится оценка разделения дерева? \n",
    "4. Зачем нужно останавливать рекурсию? \n",
    "5. Почему переобученное дерево - это плохо? \n",
    "6. Чем случайный лес отличается от ансамбля деревьев? И отличается ли? Почему важно это знать? \n",
    "7. Что делает бэггинг с решающими деревьями? \n",
    "8. Нужна ли стандартизация/нормализация признаков для деревьев? Почему?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jSDqzP2HLQf4"
   },
   "source": [
    "# Вопросики!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g7wvBJ8TLUoO"
   },
   "source": [
    "1. Чем отличаются корень, узлы и листья у дерева? Почему внезапно такая ассоциация с деревьями?\n",
    "2. Зачем нужен порог дереву? \n",
    "3. Когда в дереве много примесей - это хорошо, плохо или ещё как-то? \n",
    "4. Чем опасна рекурсия и как держать её в узде? \n",
    "5. Когда дерево может точно переобучиться? \n",
    "6. Что лучше одно супер-дерево или много нормальных таких деревьев? \n",
    "7. Как связан бэггинг и ансамблирование? \n",
    "8. Что такое бутстрэпинг? \n",
    "9. (Вопрос на расширение сознания - гляньте в инет) Что такое показатель энтропии и как его можно использовать при построении деревьев?\n",
    "10. Почему такое название \"обучение с учителем\"? Где этот учитель прячется?\n",
    "11. Можно ли переобучить модель линейной регрессии? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d6TNTwfQvce-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Pr4_RandomForest.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
