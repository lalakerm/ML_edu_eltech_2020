{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qdCNfg9CSaYD"
   },
   "outputs": [],
   "source": [
    "# Давайте установим необходимую библиотеку PyTorch\n",
    "!pip install -q torch==1.6.0\n",
    "# и импортируем ее\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W-8_Ohrjhm2Z"
   },
   "outputs": [],
   "source": [
    "# Импорт необходимых модулей \n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Настройки для визуализации\n",
    "# Если используется темная тема - лучше текст сделать белым\n",
    "TEXT_COLOR = 'black'\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (15, 10)\n",
    "matplotlib.rcParams['text.color'] = 'black'\n",
    "matplotlib.rcParams['font.size'] = 14\n",
    "matplotlib.rcParams['axes.labelcolor'] = TEXT_COLOR\n",
    "matplotlib.rcParams['xtick.color'] = TEXT_COLOR\n",
    "matplotlib.rcParams['ytick.color'] = TEXT_COLOR\n",
    "\n",
    "# Зафиксируем состояние случайных чисел\n",
    "RANDOM_STATE = 0\n",
    "np.random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GWa6Z8E2RyJ0"
   },
   "source": [
    "# Нейросети и PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5eDD4mWwR1Jk"
   },
   "source": [
    "На момент этой практики мы уже познакомились с некоторыми алгоритмами машинного обучения и даже попробовали работать с текстом. Это отличные результаты и теперь пора перейти к более узкой теме - нейросети (и обычные и глубокие). Для разработки и использования мы будем применять фреймворк PyTorch. Как всегда бывает у хорошего фреймворка, у него есть [сайт с документацией](https://pytorch.org/docs/stable/index.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YLRJfTxxTM5t"
   },
   "source": [
    "# Понятие нейрона\n",
    "\n",
    "Для освоения мы должны понять, из чего состоят нейросети, а состоят они из нейронов! Современной моделью нейрона является нейрон Розенблатта или *перцептрон*. Его можно отобразить в следующем виде:\n",
    "\n",
    "<center><img src=\"https://docs.google.com/uc?export=download&id=1pHzbK9OnHPqbqZ1MJiIie-RGk7QJji65\"/></center>\n",
    "\n",
    "В нейроне можно выделить следующие части:\n",
    "- **Входы** - это линии, по которым нейрон получает сигналы из, например, выходов других нейронов.\n",
    "\n",
    "- **Веса** - это коэффициенты, которые показывают, насколько сигнал конкретного входа важен для нейрона. Видите, у каждого входа свой вес. По аналогии с линейной или логистической регрессией, веса - это параметры нейрона, которые меняются (обучаются) в ходе обучения. Также по аналогии с линейной регрессией здесь есть и *bias* ($b$).\n",
    "\n",
    "- **Сумматор** - этап объединения взвешенных входов. Какого-то особого смысла не имеет, можно также провести аналогию с моделью линейной регрессии.\n",
    "\n",
    "- **Функция активации** - вот тут и кроется самая важная часть нейрона. До данного момента все этапы имели линейный характер. Если мы сделаем нейросеть только из нейронов без функции активации, то вся нейросеть будет иметь линейный характер и ее ценность будет не больше любой линейной модели. Чаще всего функция активации - нелинейная функция. Мы уже ознакомили с одной из распространенных функций, которая используется в нейросетях - сигмоида. Помимо этого есть и другие, отобразим парочку из них:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 361
    },
    "executionInfo": {
     "elapsed": 4385,
     "status": "ok",
     "timestamp": 1602520181711,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "GMwc5ZsnW-MA",
    "outputId": "7f45ec63-66aa-465e-ea27-37da650d9f16"
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-10, 10, 100)\n",
    "\n",
    "plt.figure(figsize=[20,5])\n",
    "plt.subplot(141)\n",
    "plt.plot(x, 1 / (1 + np.exp(-x)))\n",
    "plt.title('Сигмоида (sigm)')\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(142)\n",
    "plt.plot(x, np.tanh(x))\n",
    "plt.title('Гиперболический тангенс (tanh)')\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(143)\n",
    "plt.plot(x, np.maximum(x, 0))\n",
    "plt.title('ReLU - Rectified Linear Unit')\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(144)\n",
    "plt.plot(x, x>0)\n",
    "plt.title('Пороговая функция')\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDm_BQZMYOyg"
   },
   "source": [
    "Вот тут мы и подошли к одному из важнейших факторов, которые отличают нейросети от остальных моделей. Нейрон в своей минималистичности (сумма взвешенных входов) с нелинейной функцией активации уже является *нелинейной* моделью. Если мы сложим из таких нейронов нейросеть, то получим **универсальный аппроксиматор**. Сложно? Проще - Какую бы хитрую вы функцию не выдумали (квадрат косинуса от корня экспоненты в степени икс), можно взять нейросеть достаточного размера (по количеству нейронов в ней) и она сможет аппроксимировать ее, то есть повторить зависимость $y=f(x)$.\n",
    "\n",
    "## Формулы для нейрона\n",
    "\n",
    "Теперь, когда мы разобрали, из чего нейрон состоит, можно перейти к математическому описанию - глянем на формулы.\n",
    "\n",
    "Модели (функцию предсказания) мы описывали в виде $\\hat{y} = h_W(X)$. В нашем случае $h_W(X) = g(XW)$, где $g(z)$ - функция активации. Какую функцию активации выбрать? Над этим вопросом бьются ученые со всего мира, какая же всё-таки лучше, так как выбор влияет на работу сети очень сильно.\n",
    "\n",
    "> Тип функции активации является гиперпараметром нейрона и нейросети.\n",
    "\n",
    "Не забывайте, что в нейроне есть *bias* ($b$). Суть его точно такая же, как в линейной регрессии. Чтобы его описать, мы точно также добавляем в $X$ колонку единиц и $W$ в нейроне - вектор с размером $(M+1)$ ($M$ - количество входов) нейрона.\n",
    "\n",
    "> Мы в этой части не будем писать код, потому что вы уже делали подобные вещи, но крайне рекомендуем почитать материал, так как на нем строятся дальнейшие выводы.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXM3k6h5TwM0"
   },
   "source": [
    "\n",
    "## Обучение нейрона\n",
    "\n",
    "> На этом моменте мы не заостряем внимание. Основная причина в том, что на сегодняшний день написание своей логики обучения нужно либо для исследовательских целей, либо для очень глубокого погружения, и своя реализация с нуля может иметь очень много ошибок. Тем не менее, вопросы обучения очень широки, поэтому крайне рекомендуем ознакомиться с различными методами и подходами в дополнительных материалах.\n",
    "\n",
    "Учится нейрон очень похожим образом с другими моделями по методу градиентного спуска. Главная задача в обучении - найти такие значения весов, чтобы выход нейрона максимально соответсвовал данным, на которых обучается (не забываем, что здесь не учитывается шум в данных - слишком сильное соответсвие шуму может привести к переобучению, что очень плохо).\n",
    "\n",
    "Как обычно, градиентный спуск работает по следующему принципу\n",
    "$$\n",
    "W := W - \\alpha \\frac{\\partial J(W)}{\\partial W}\n",
    "$$\n",
    "\n",
    "$W$ - веса нейрона, $J(W)$ - функция потерь (как обычно, выбирается в зависимости от задачи).\n",
    "\n",
    "Для примера регрессии, возьмем функцию $MSE$ в качестве функции потерь и рассмотрим случай с одной записью (чтобы не писать громоздкую сумму и остальное):\n",
    "$$\n",
    "J(W) = (y-h_W(x))^2 = (y-g(x*W))^2\n",
    "$$\n",
    "\n",
    "> Тут маленький $x$, потому что мы рассматриваем конкретную запись в данных, а не весь набор.\n",
    "\n",
    "Так вот, у нас в формуле появилась функция активации нейрона, которая имеет нелинейный характер. Что нам с ней делать? Просто расписать градиент по цепочке:\n",
    "$$\n",
    "\\frac{\\partial J(W)}{\\partial W} =\n",
    "\\frac{\\partial J(W)}{\\partial g(z)} \\frac{\\partial g(z)}{\\partial z} \\frac{\\partial z}{\\partial W}\n",
    "$$\n",
    "\n",
    "Распишем производные, ведь мы уже находили их:\n",
    "$$\n",
    "\\frac{\\partial J(W)}{\\partial g(z)} = -(y-g(z)) \\\\\n",
    "\\frac{\\partial g(z)}{\\partial z} = g(z)*(1-g(z)) \\\\\n",
    "\\frac{\\partial z}{\\partial W} = X^T\n",
    "$$\n",
    "\n",
    "Тогда можно записать полную формулу для градиента:\n",
    "$$\n",
    "\\frac{\\partial J(W)}{\\partial W} = X^T*(g(XW)*(1-g(XW))*(g(XW)-y))\n",
    "$$\n",
    "\n",
    "Таким образом, нам нужно еще и для каждой функции активации знать ее градиент, но это не сложно, мы уже искали градиент сигмоиды, остальные тоже можно найти.\n",
    "\n",
    "Дальше как обычно, обновляем веса с коэффициентом обучения $\\alpha$ и идем к следующей итерации.\n",
    "\n",
    "Все остальные шаги уже делались, так что обучение отдельного нейрона не составляет сложности! Поехали дальше!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-8cRRkTfRg1"
   },
   "source": [
    "# Нейросеть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrrRUmxQfbSU"
   },
   "source": [
    "Отлично, нейрон мы освоили, но если сказать честно, сегодня единственный нейрон нигде не применяется. Это не значит, что он бесполезен! Давайте возьмем пачку таких нейронов и выстроим в слои:\n",
    "\n",
    "> Нейросеть (по аналогии с мозгом) - это много соединенных нейронов между собой, поэтому для большинства нейронов выходные сигналы идут на вход другим нейронам.\n",
    "\n",
    "> Разработка нейросети на компьютере делает нейросеть **искусственой**, поэтому сигналами являются передаваемые числа (или уровень сигнала).\n",
    "\n",
    "<center><img src=\"https://docs.google.com/uc?export=download&id=1T3RURXveKUqEdlljOHYqs1Aorav5uCRa\"/></center>\n",
    "\n",
    "Целых три слоя, судя по картинке, но давайте разберемся, почему же реальных слоев тут всё-таки *два* =)\n",
    "\n",
    "- **Input layer (входной слой)** - обратим внимание, что каждый \"нейрон\" здесь имеет единственный вход. Выход на самом деле у каждого нейрона в слое один, просто он идет на все нейроны в следующем слое. \n",
    "\n",
    "> Такая форма сети называется **полносвязной** - когда мы соединяем все выходы предыдущего слоя со входами следующего.\n",
    "\n",
    "- **Hidden layer (скрытый слой)** - он называется скрытым, потому что он не вход и не выход. Все что между - скрытые слои. Каждый нейрон в этом слое имеет столько входов, сколько нейронов у нас в предыдущем слое. В данном случае мы имеем по два входа у каждого нейрона этого слоя.\n",
    "\n",
    "> Помним, что у нейрона несколько входов, единственный bias и единственный выход.\n",
    "\n",
    "- **Output layer (выходной слой)** - тут особенность обратная входному, полносвязное соединение с предыдущим скрытым и по одному выходу на нейрон. Но ведь у нейрона и так всегда один выход? Да, просто дальше слоев нет, поэтому на рисунке нет столько стрелочек на выходе.\n",
    "\n",
    "Теперь, почему тут на самом деле два слоя нейронов, а не три? Помним, что нейрон - это веса для входов и *bias*, суммирование и функция активации? Так вот у входных \"нейронов\" как правило нет весов и функции активации. Ну и суммировать нечего, если в входном слое один нейрон - один вход. Это схематичное отображение того, что каждый нейрон в первом скрытом слое берет все значения входов. Так просто на схемах проще показывать.\n",
    "\n",
    "> А что является входом для нейросети? Да как и для всех моделей в ML - числовые значения признаков!\n",
    "\n",
    "Такая структура называется **многослойной**. Для случая, когда у нас нет скрытого слоя, то остается единственный слой с весами - **однослойная структура**. Хотим сделать два скрытых? Не вопрос - делайте! =)\n",
    "\n",
    "> Архитектура нейросети (количество слоев и количество нейронов в слое) является гиперпараметром модели.\n",
    "\n",
    "## Функция предсказания нейросети\n",
    "\n",
    "Переходя к функции предсказания, мы должны понять, что в нейросети каждый нейрон имеет свои веса. Давайте для примера рассмотрим скрытый слой, там мы имеем 3 нейрона, по 2 входа у каждого нейрона (так как на предыдущем \"слое\" у нас два нейрона). Таким образом, каждый нейрон имеет вектор размерностью $(3)$ и мы можем разместить эти вектора в матрицу, чтобы это была матрица весов скрытого слоя (на схеме $W_{ih}$ - размер $(2+1, 3)$). Аналогичным образом строим матрицу весов для выходного слоя ($W_{ho}$), 2 нейрона, 3 входа у каждого - размер $(3+1, 2)$\n",
    "\n",
    "> Вектор весов складывается из весов для каждого входа + веса для входа bias, который всегда равен единице.\n",
    "\n",
    "Теперь, когда мы понимаем, что отдельные нейроны в слое не рассматриваются, а мы рассматриваем весь слой и описываем его матрицей, то стоит понять еще одну важную вещь: функция активации берется для всего слоя одна. То есть, если мы приняли для конкретного слоя функцию активации *сигмоида*, то все нейроны в этом слое должны применять ее к выходам.\n",
    "\n",
    "После таких умозаключений давайте сформируем функцию предсказания для слоев (индекс сверху):\n",
    "$$\n",
    "h_W^{h}(X) = g^{h}(XW) \\\\\n",
    "h_W^{o}(X) = g^{o}(XW)\n",
    "$$\n",
    "\n",
    "Обратите внимание, это просто формулы, но они одинаковы, если мы рассматриваем каждый слой по отдельности без связи в нейросети и со своей функцией активации. Так а как будет выглядеть функция предсказания нейросети конкретно для нашего случая? Не пугаемся, но вот она:\n",
    "$$\n",
    "h(X) = h_W^{o}(h_W^{h}(X)) = g^{o}(g^{h}(X*W_{ih}) * W_{ho})\n",
    "$$\n",
    "\n",
    "Тут на самом деле нет ничего сложного, просто вложенность одного в дургое. По сути цепочка строится таким образом:\n",
    "- Вычисляется произведение $XW$ для первого слоя (со своей матрцией весов);\n",
    "- Применяется функция активации первого слоя $g(x)$;\n",
    "- Полученный результат передается на второй слой как вход $X$;\n",
    "- Продолжаем до конца слоев в нейросети.\n",
    "\n",
    "То есть, никто не пишет обычно полную функцию активации - все понимают, что вычисление происходит последовательно от слоя к слою. Вот мы и разобрались с тем, как нейросеть вычисляет предсказания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ywj-eY5yo814"
   },
   "source": [
    "## Обучение нейросети\n",
    "\n",
    "Обучение нейросети тоже происходит по алгоритму градиентного спуска, но никто не запрещает пользоваться другими алгоритмами оптимизации (\n",
    "Алгоритм Левенберга — Марквардта, генетические алгоритмы, вариации алгоритма градиентного спуска и др.). Правда процесс обучения немного не классический. Функция предсказания у нас вычисляет результат от слоя к слою - такой процесс называется **прямое распространение (forward feed)**. Когда нейросеть вычислила первый раз результат во время обучения, мы можем сравнить результат с истинным значением и получить отклонения предсказания (ошибку). Тогда мы запускаем \"некоторую *волну*\", которая распространяет эту ошибку в обратном порядке от слоя к слою, чтобы на каждом слое обновить веса - это **обратное распространение (backpropagation)** ошибки.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://docs.google.com/uc?export=download&id=1sdEgzwmkXEQfbDM2CjXXrveB-K30m2iD\"/>\n",
    "</center>\n",
    "\n",
    "\n",
    "> Мы рассматривать типы нейросетей особо не будем - есть очень много классификаций на различных ресурсах, но обратите внимание на забавный факт! В рассматриваемой сети во время обучения (*train mode*) происходит два направления распространения информация, прямой ход и обратный. В режиме предсказания (*inference mode*) сеть работает полько применяя прямой ход информации. Поэтому такие сети называются *сети прямого распространения (feedforward networks)*. В качестве сравнения существуют рекуррентные сети, в которых информация в режиме предсказания распространяется не только прямым ходом - этакий режим памяти.\n",
    "\n",
    "На всякий случай вспомним, что обновление веса по градиентному спуску происходит по формуле:\n",
    "$$\n",
    "w_{o_1} \\leftarrow w_{o_1} - \\alpha \\frac{\\partial J}{\\partial w_{o_1}^1}\n",
    "$$\n",
    "\n",
    "Так как же делается это обратное распространение ошибки? Мы для этого выделим конкретную ветку и посмотрим, что и как расчитывается. \n",
    "\n",
    "Начнем с последнего слоя:\n",
    "\n",
    "<center> \n",
    "<img src=\"https://docs.google.com/uc?export=download&id=1YloSxTLhIE_C2lUxW-uV7dzoVCGKRl9J\"/>\n",
    "</center>\n",
    "\n",
    "Здесь принцип обучения не отличается от принципа обучения простого нейрона. Формулы все те же, для примера возьмем конкретный вес:\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w_{o_1}^1} = \n",
    "\\frac{\\partial J}{\\partial g_{o_1}} \n",
    "\\frac{\\partial g_{o_1}}{\\partial z_{o_1}} \n",
    "\\frac{\\partial z_{o_1}}{\\partial w_{o_1}^1} = \n",
    "(g(z_{o_1})-y)*g(z_{o_1})*(1-g(z_{o_1}))*a_{h_1}\n",
    "$$\n",
    "\n",
    "> $a_{h_1}$ - выход нейрона $h_1$ и, соответственно, вход для нейрона $o_1$\n",
    "\n",
    "Обратите внимание, чтобы нам получить производную по весу последнего нейрона, нам нужно развернуть \"цепочку\" вычислений в обратную сторону по всем операциям.\n",
    "\n",
    "В данном случае получить обновление не сложно, давайте перейдем к разбору того, а как добраться до весов в более глубоких слоях и попытаемся понять основную идею распространения!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J41zZP6wdUwi"
   },
   "source": [
    "\n",
    "Теперь рассмотрим такой кейс:\n",
    "\n",
    "<center>\n",
    "<img src=\"https://docs.google.com/uc?export=download&id=16A1u4RGgo6RIljet51UvjNodRfZxrLEl\"/>\n",
    "</center>\n",
    "\n",
    "Здесь мы смотрим на конкретный вес, так как для всех остальных принцип будет тот же. Наша задача найти проихводную $\\frac{\\partial J}{\\partial w_{h_1}^1}$, то есть, как надо поменять вес $w_{h_1}^1$, чтобы функцию потерь мы уменьшали.\n",
    "\n",
    "Давайте для удобства запишем нынешний вариант формулы для вычисления:\n",
    "$$\n",
    "\\hat{y} = g_{o_1}(z_{o_1}) \\\\\n",
    "z_{o_1} = a_{h_1}*w_{o_1}^1 \\\\\n",
    "a_{h_1} = g_{h_1}(z_{h_1}) \\\\\n",
    "z_{h_1} = x_{1}*w_{h_1}^1\n",
    "$$\n",
    "\n",
    "Теперь мы можем полностью развернуть нашу прозводную, пройдя по двум нейронам (сначала разворачиваем скрытый нейрон, затем выходной):\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w_{h_1}^1} = \n",
    "\\frac{\\partial J}{\\partial g_{h_1}}\n",
    "\\frac{\\partial g_{h_1}}{\\partial z_{h_1}} \n",
    "\\frac{\\partial z_{h_1}}{\\partial w_{h_1}^1} =\n",
    "\\frac{\\partial J}{\\partial g_{o_1}} \n",
    "\\frac{\\partial g_{o_1}}{\\partial z_{o_1}} \n",
    "\\frac{\\partial z_{o_1}}{\\partial g_{h_1}} \n",
    "\\frac{\\partial g_{h_1}}{\\partial z_{h_1}} \n",
    "\\frac{\\partial z_{h_1}}{\\partial w_{h_1}^1}\n",
    "$$\n",
    "\n",
    "А для выходного слоя была формула такая:\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w_{o_1}^1} = \n",
    "\\frac{\\partial J}{\\partial g_{o_1}} \n",
    "\\frac{\\partial g_{o_1}}{\\partial z_{o_1}} \n",
    "\\frac{\\partial z_{o_1}}{\\partial w_{o_1}^1}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTJGumZ8KSx-"
   },
   "source": [
    "Давайте присмотримся и постараемся разобраться. При выводе мы всегда сталкиваемся с такой производной:\n",
    "- Для $w_{o_1}$ мы сталкиваемся с $\\frac{\\partial J}{\\partial g_{o_1}} \n",
    "\\frac{\\partial g_{o_1}}{\\partial z_{o_1}}$;\n",
    "- Для $w_{h_1}$ мы сталкиваемся с $\\frac{\\partial J}{\\partial g_{h_1}} \n",
    "\\frac{\\partial g_{h_1}}{\\partial z_{h_1}}$;\n",
    "\n",
    "При этом эта часть всегда умножается на производную, которая в результате дает просто значение входа нейрона. А давайте запишем ее вот так:\n",
    "$$\n",
    "\\delta_{o_1} = \\frac{\\partial J}{\\partial g_{o_1}} \\frac{\\partial g_{o_1}}{\\partial z_{o_1}} \\\\\n",
    "\\delta_{h_1} = \\frac{\\partial J}{\\partial g_{h_1}} \\frac{\\partial g_{h_1}}{\\partial z_{h_1}}\n",
    "$$\n",
    "\n",
    "Тогда получается, что для каждого нейрона уравнение производной получается следующим:\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w_{o_1}^1} = \\delta_{o_1}*\\frac{\\partial z_{o_1}}{\\partial w_{o_1}^1} \\\\\n",
    "\\frac{\\partial J}{\\partial w_{h_1}^1} = \\delta_{h_1}*\\frac{\\partial z_{h_1}}{\\partial w_{h_1}^1}\n",
    "$$\n",
    "\n",
    "А так как $z$ у любого нейрона - это взвешенная сумма входов, то производная будет давать просто вход нейрона по этому весу (по которому берется производная)! То есть определение производной превратилось в нахождение дельты умноженной на вход нейрона. Давайте запомним дельту как **ошибка нейрона**.\n",
    "\n",
    "> Уточним, под входом нейрона понимается значение, которое было подано на вход во время прямого прохода.\n",
    "\n",
    "Так, уже выглядит проще! А теперь главный вопрос, как найти эту дельта? Здесь нам поможет длинный вывод производной для нейрона в скрытом слое:\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial g_{h_1}} \n",
    "\\frac{\\partial g_{h_1}}{\\partial z_{h_1}} = \n",
    "\\frac{\\partial J}{\\partial g_{o_1}} \n",
    "\\frac{\\partial g_{o_1}}{\\partial z_{o_1}} \n",
    "\\frac{\\partial z_{o_1}}{\\partial g_{h_1}}\n",
    "\\frac{\\partial g_{h_1}}{\\partial z_{h_1}} \n",
    "$$\n",
    "\n",
    "или\n",
    "\n",
    "$$\n",
    "\\delta_{h_1} = \\delta_{o_1} * \\frac{\\partial z_{o_1}}{\\partial g_{h_1}} * \\frac{\\partial g_{h_1}}{\\partial z_{h_1}} \n",
    "$$\n",
    "\n",
    "Что означает такая запись? Да именно то, что ошибку последующих нейронов (по глубине от выхода ко входу) мы можем рассчитать через ошибку предыдущих. Здесь в примере ошибка скрытого нейрона рассчитывается на основе ошибки выходного нейрона. И тут на секунду, ошибку выходного нейрона мы уже точно умеем рассчитывать! Круто! Осталось до конца разобраться с этой формулой!\n",
    "\n",
    "Для начала, посмотрим на эту часть $\\frac{\\partial z_{o_1}}{\\partial g_{h_1}}$ - это та часть, которая связывает взвешенную сумму $z_{o_1}$ нейрона и его вход (выход предыдущего нейрона $g_{h_1} = a_{h_1}$), то есть просто вес связи $w_{o_1}^1$!\n",
    "\n",
    "Ну а выражение $\\frac{\\partial g_{h_1}}{\\partial z_{h_1}}$ - просто производная функции активации предыдущего нейрона. Тоже ничего специфичного.\n",
    "\n",
    "То есть, фактически, уравнение:\n",
    "$$\n",
    "\\delta_{h_1} = \\delta_{o_1} * \\frac{\\partial z_{o_1}}{\\partial g_{h_1}} * \\frac{\\partial g_{h_1}}{\\partial z_{h_1}}\n",
    "$$\n",
    "\n",
    "это получение ошибки следующего (при обратном распространении) слоя за счет умножения ошибки нейрона и производной функции активации! \n",
    "\n",
    "Такими шагами можно добраться до самых первых слоев даже для 100-слойной сети, но это уже совсем другая история про **затухание/взрыв градиента** и **влияние функции активации на распространение градиента**.\n",
    "\n",
    "> Если вам интересно, что это за эффекты - обязательно прочтите про них!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w10_hWsNfjJk"
   },
   "source": [
    "## Для более интересующихся\n",
    "\n",
    "Мы сейчас разбирались в терминах производных, но вы можете проверить себя и сформировать формулы производных для конкретных случаев. Например, для рассматриваемых производных были такие результаты:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial g_{o_1}} = g(z_{o_1})-y \\\\\n",
    "\\frac{\\partial g_{o_1}}{\\partial z_{o_1}} = g(z_{o_1})*(1-g(z_{o_1}) \\\\\n",
    "\\frac{\\partial z_{o_1}}{\\partial g_{h_1}} = \\frac{\\partial z_{o_1}}{\\partial a_{h_1}} = w_{o_1}^1 \\\\\n",
    "\\frac{\\partial g_{h_1}}{\\partial z_{h_1}} = g(z_{h_1})*(1-g(z_{h_1}) \\\\\n",
    "\\frac{\\partial z_{h_1}}{\\partial w_{h_1}^1} = x_1\n",
    "$$\n",
    "\n",
    "И сформируем полное уравнение:\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w_{h_1}^1} = \n",
    "(g(z_{o_1})-y)*g(z_{o_1})*(1-g(z_{o_1}))*w_{o_1}^1 *\n",
    "g(z_{h_1})*(1-g(z_{h_1})*x_1\n",
    "$$\n",
    "\n",
    "Для сравнения:\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w_{o_1}^1} = \n",
    "(g(z_{o_1})-y)*g(z_{o_1})*(1-g(z_{o_1}))*a_{h_1}\n",
    "$$\n",
    "\n",
    "Но мы уже выяснили, что производные вычисляются через ошибки, поэтому полные формулы даже второго скрытого с конца слоя никто не хочет смотреть =)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EEFWTSXqSOlb"
   },
   "source": [
    "\n",
    "На этом мы заканчиваем нашу чисто теоретическую часть, так как на сегодняшний день мы можем пользоваться благами готовых инструментов, понимая, как это все работает!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pjo2XyO0Si5v"
   },
   "source": [
    "# Первые шаги в PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27V64bcwYbN3"
   },
   "source": [
    "## Тензоры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_sSIsg1QSlwN"
   },
   "source": [
    "Свое знакомство с нейросетями мы начнем с освоения фреймворка PyTorch. Тут нет ничего сложного, просто надо понять, что PyTorch привык работать со своим форматом массивов, поэтому мы встречаемся с первым понятием - **тензор**. По сути своей тензор ничем не отличается от массива, тоже может иметь любое количество размерностей, но PyTorch имеет свой класс для этого, поэтому надо привыкать работать с ним.\n",
    "\n",
    "Давайте создадим тензор единиц размером $(3, 4)$, при этом мы изначально создадим массив через numpy и затем передадим его в функцию `torch.tensor()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "moRPQnvNUdZ3"
   },
   "outputs": [],
   "source": [
    "# TODO - создадим массив numpy размером (3, 4) и преобразуем в тензор\n",
    "# NOTE - в помощь справка https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor\n",
    "\n",
    "ones_numpy = None\n",
    "ones_tensor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xX_ZVfypUuWD"
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "\n",
    "# Да, у тензора тоже есть атрибут .shape\n",
    "assert np.all(ones_tensor.shape == np.array([3, 4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A0FrBRZbXCFb"
   },
   "source": [
    "Можно было сразу создать тензор единиц с помощью `torch.ones()`, но тут важно понять, что numpy и torch могут тесно взаимодействовать и переводить данные из одного формата в другой! Давайте вернемся из тензора в формат numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xGMTmxNxXRjy"
   },
   "outputs": [],
   "source": [
    "# TODO - получите массив numpy с помощью метода тензора `torch.Tensor.numpy()`\n",
    "data_tensor = torch.tensor([[3, 4], [1, 2]])\n",
    "data_numpy = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_e1DCPwuXj3x"
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "assert isinstance(data_numpy, (np.ndarray))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NjfhtHSSWq_2"
   },
   "source": [
    "По сути своей тензоры идентичны матрицам, но есть две важные особенности:\n",
    "- если присмотреться к аргументу `requires_grad`, то можно выяснить, что тензоры хранят не только значения, но и величины градиентов! Эти градиенты используются при обратном распространении ошибки системой `autograd` в PyTorch.\n",
    "- также имеется аргумент `device`, который управляет тем, на каком вычислителе будут производиться операции. Да-да, вот так просто можно кинуть данные на видеокарту (если есть NVidia) и выполнить операции, но не думайте, что таким образом достигается лучшая производительность. Видеокарта умеет быстро выполнять огромные объемы простых вычислений, а кидать парочку массивов на сложение - не выгодно и будет дольше, чем на CPU!\n",
    "\n",
    "В этом все отличия тензора, по всем операциям вы можете посмотреть [доки](https://pytorch.org/docs/stable/index.html), а мы двигаемся дальше!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4Mrl8DMYcph"
   },
   "source": [
    "## Модули"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EPFDURCYYeCy"
   },
   "source": [
    "**Модули** в PyTorch - это классы, которые имеют определенный функционал обработки. Их еще можно назвать операциями. Например, есть модуль $MSE$, модуль сигмоиды, модуль Softmax и т.д.\n",
    "\n",
    "В чем их отличие от простых функций или классов? При разработке нейросетей, например, на основе функции потерь вычисляются градиенты, которые дальше распространяются для обновления весов. Простые функции или операции нам только дадут конкретные значения, а использование модулей в PyTorch позволяет не только запомнить, как распространялись по сети вычисления, но и вычислить необходимые градиенты.\n",
    "\n",
    "Другой важной особенностью является абстракция аппаратного кода. Мы не задумываемся, как производить вычисления на GPU или CPU, при этом в тензоре можно задать целевую аппаратуру для использования. Модули в этом плане хороши тем, что при выполнении операций также смотрят на аппаратуру переданных тензоров и выполняют операции на той аппаратуре, на которую нацелены тензоры (заданы через `device`). Таким образом, абстракция заключается в том, что мы не пишем отдельный код, а пользуемся одними и теми же модулями без каких-либо проблем.\n",
    "\n",
    "Давайте теперь перейдем к знакомству и первым мы познакомимся с модулем линейного слоя. На деле это слой нейронов, но без функции активации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C9yvBuSMyQFm"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
    "neurons_layer = nn.Linear(\n",
    "    in_features=3,  # Количество нейронов в предыдущем слое\n",
    "                    #   или количество входов нейросети, если слой первый\n",
    "    out_features=2  # Количество нейронов в слое (выходов слоя)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rH4newKVyuG6"
   },
   "source": [
    "Как всегда, данный класс имеет атрибуты. Подробнее можем глянуть в доках, а пока посмотрим на веса:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "executionInfo": {
     "elapsed": 4283,
     "status": "ok",
     "timestamp": 1602520181717,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "S_57ugCERfBK",
    "outputId": "8da2cd1f-0a29-4e78-9f2b-6073379601eb"
   },
   "outputs": [],
   "source": [
    "neurons_layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 4259,
     "status": "ok",
     "timestamp": 1602520181718,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "ObEJELo4y42h",
    "outputId": "c18124c2-3532-4093-db7c-ada47b61fccc"
   },
   "outputs": [],
   "source": [
    "neurons_layer.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13eUa60jzpcA"
   },
   "source": [
    "Как видим, веса модуля тоже представлены тензором. При этом есть возможность отключить веса `bias`, чтобы слой имел только веса для входов, без смещения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mEBR12Me04iv"
   },
   "outputs": [],
   "source": [
    "# TODO - создайте слой нейронов и отключите bias\n",
    "\n",
    "neurons_layer_no_bias = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Su2_mU01Dr9"
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "assert neurons_layer_no_bias.bias is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xaFTZoUf01Y_"
   },
   "source": [
    "Хорошо, мы смогли создать слой с `bias`, без него, но как им пользоваться? Очень просто! Модуль - это конкретная операция, поэтому все, что нам нужно - вызвать его!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "executionInfo": {
     "elapsed": 4191,
     "status": "ok",
     "timestamp": 1602520181720,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "R1FkRvXy0Qhu",
    "outputId": "d1e73b22-7552-4b0a-92b6-3c5c6e62e646"
   },
   "outputs": [],
   "source": [
    "# Слой в три нейрона и двумя входами (данные или предыдущий слой)\n",
    "layer = nn.Linear(3, 2)\n",
    "# Зададим свои веса модели\n",
    "layer.weight.data.fill_(1)\n",
    "layer.bias.data.fill_(1)\n",
    "# Создаем пример данных с одной записью и тремя признаками\n",
    "input = torch.tensor([[1.5, 2, 3]])\n",
    "print(f'Input: {input}')\n",
    "print(f'Weights: {layer.weight}')\n",
    "print(f'Bias: {layer.bias}')\n",
    "\n",
    "result = layer(input)\n",
    "print(f'\\тResult: {result}')\n",
    "\n",
    "manual_result = input @ layer.weight.T + layer.bias\n",
    "print(f'Manual result: {manual_result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KYIXU5Vd4PCW"
   },
   "source": [
    "Шикарно, мы использовали модуль и затем проверили, что все работает корректно!\n",
    "\n",
    "Смотрите, в тензоре еще есть атрибут `grad_fn`. На самом деле это как раз специфика модулей и применяемых тензоров. При использовании модуля он сразу вычисляет необходимые для градиента данные, а тензор может хранить информацию об этом градиенте, чтобы в дальнейшем использовать ее для распространения ошибки. Вот так красиво можно не волноваться о градиенте, а просто заниматься разработкой кода и анализом данных!\n",
    "\n",
    "Пора бы нам уже обучить нашу первую нейросеть на PyTorch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "URPdTCFth1NU"
   },
   "source": [
    "# Веса слоя нейросети - их первые значения\n",
    "\n",
    "Инициализация весов - это вообще очень большой вопрос! Мы его здесь рассматривать не будем, но обратите внимание, что PyTorch решает эту проблему за вас! Проблема называется **проблема симметрии** и она очень сильно влияет на работу нейросети. Об этом с примером можно прочитать [здесь](https://www.quora.com/Why-dont-we-initialize-the-weights-of-a-neural-network-to-zero) и на многих других ресурсах! Именно поэтому при работе веса часто инициализируются случайными числами из некоторого распределения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EyYYJOPh-ZNu"
   },
   "source": [
    "# Наша первая нейросеть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXSJYhKaVqgL"
   },
   "source": [
    "Сейчас мы создадим пачку точек (данных), чтобы научиться строить и работать с ними в PyTorch!\n",
    "\n",
    "> Не забывайте, что реальная работа с данными и обучение моделей предполагает разделение на выборки:\n",
    "- обучения - для тренировки моделей;\n",
    "- валидация - для настройки гиперпараметров модели;\n",
    "- тест - для окончательной оценки модели.\n",
    "\n",
    "> В данной практике мы пропустим вопросы предобработки данных, тем не менее учитывайте, что использование модели в виде нейросети не сильно отличается в отношении подготовки данных и стандартизации!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 632
    },
    "executionInfo": {
     "elapsed": 4160,
     "status": "ok",
     "timestamp": 1602520181721,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "pyJ3oxb2AcE0",
    "outputId": "3230fa7e-2e13-4db5-f13f-af17f2b40fcc"
   },
   "outputs": [],
   "source": [
    "n_points = 100\n",
    "\n",
    "real_W = [2, 0.7]\n",
    "X_data = 4*np.sort(np.random.rand(n_points, 1), axis=0)+1\n",
    "noize = 1*(np.random.rand(n_points, 1)-0.5)\n",
    "y_data_true = real_W[0] + real_W[1]*X_data\n",
    "y_data_noized = y_data_true + noize\n",
    "y_data = y_data_noized[:, 0]\n",
    "\n",
    "X_render = np.linspace(X_data[:, 0].min(), X_data[:, 0].max(), 100)\n",
    "y_render = real_W[0] + real_W[1]*X_render\n",
    "\n",
    "plt.scatter(X_data, y_data_noized, label='Данные')\n",
    "plt.ylabel('$y$')\n",
    "plt.xlabel('$x$')\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2vBVdqFD-bQn"
   },
   "source": [
    "Начнем как всегда с простого, получение предсказаний и оценка (визуальная и численная):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "executionInfo": {
     "elapsed": 4128,
     "status": "ok",
     "timestamp": 1602520181722,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "KIm6eHwV1-cx",
    "outputId": "293ba6f4-121e-4c3a-a46d-38f67273175d"
   },
   "outputs": [],
   "source": [
    "# Мы будем добавлять строку задания seed при каждом создании слоя,\n",
    "#   чтобы зафиксировать случайные числа при инициализации весов\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "\n",
    "# Создаем слой из одного нейрона в слое и одного входа\n",
    "#   У слоя не будет активации (или можно назвать \"линейной\" активацией)\n",
    "model = nn.Linear(1, 1)\n",
    "# Переводим данные в формат тензора\n",
    "#   Приводим к типу float методом .float()\n",
    "X_tnsr = torch.tensor(X_data).float()\n",
    "y_true_tnsr = torch.tensor(y_data).float()\n",
    "\n",
    "# Исполняем слой - получаем предсказание\n",
    "y_pred_tnsr = model(X_tnsr)\n",
    "\n",
    "# Создаем модуль оценки loss методом MSE и оцениваем\n",
    "loss_mod = nn.MSELoss()\n",
    "loss_value = loss_mod(y_pred_tnsr, y_true_tnsr)\n",
    "\n",
    "print(loss_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NCVlcienG9LF"
   },
   "source": [
    "Глядите, результат оценки loss - это тоже тензор одиночного значения с информацией о градиенте! Если нам нужно использовать его с другими тензорами, то так и оставляем, а для того, чтобы просто получить скалярное значение, то можно воспользоваться методом `.item()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 4098,
     "status": "ok",
     "timestamp": 1602520181722,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "R3Mo8ahhHVLj",
    "outputId": "78633b7b-58e0-4bb7-e702-74bbc99cd65e"
   },
   "outputs": [],
   "source": [
    "loss_scalar = loss_value.item()\n",
    "print(f'Scalar value: {loss_scalar}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZQMqLKWHcFA"
   },
   "source": [
    "Отлично, эти методы полезны, когда мы работаем с тензорами! Теперь осталось посмотреть на наши данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tB3apwjGGZy7"
   },
   "outputs": [],
   "source": [
    "def plot_model(X, y_pred, y_true):\n",
    "    plt.scatter(X, y_true, label='Данные')\n",
    "    plt.plot(X, y_pred, 'k--', label='Предсказание модели')\n",
    "    plt.ylabel('$Y$')\n",
    "    plt.xlabel('$X$')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 614
    },
    "executionInfo": {
     "elapsed": 4056,
     "status": "ok",
     "timestamp": 1602520181724,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "F9VOib-bBktJ",
    "outputId": "6e5d6277-4ba1-4ecb-cc44-fba17270711d"
   },
   "outputs": [],
   "source": [
    "# Переводим обратно в формат numpy\n",
    "#   .detach() нужен, чтобы отсоединить информацию о градиенте\n",
    "#   без этого будет ошибка - можете попробовать проверить\n",
    "y_pred = y_pred_tnsr.detach().numpy()\n",
    "\n",
    "plot_model(X_data, y_pred, y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ongYL8sILSX"
   },
   "source": [
    "Как и ожидалось, случайные веса в нейросети не дают ожидаемого результата. Пора научиться обучать сеть!\n",
    "\n",
    "> В PyTorch с фиксацией случайных чисел есть неопределенности, поэтому от запуска к запуску веса, а соответсвенно и предсказания, могут отличаться - не пугайтесь.\n",
    "\n",
    "Что мы сейчас знаем:\n",
    "- Как преобразовать данные в формат тензора\n",
    "- Как создать однослойную нейронную сеть (пока что один нейрон в слое)\n",
    "- Как оценить функцию потерь встроенными инструментами\n",
    "- Как преобразовывать данные из тензора в специальных случаях (отсоединение градиента и получение скаляра)\n",
    "\n",
    "> Если вы чего-то из этого списка не помните - лучше вернуться и повторить!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NxoLvCYRJF5L"
   },
   "source": [
    "## Обучение нейросети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxdV54zmJIAK"
   },
   "source": [
    "Хорошо, предсказания и оценка Loss получены, теперь остался вопрос, а как в PyTorch сделать обновление весов?\n",
    "\n",
    "Для начала глянем на правило градиентного спуска:\n",
    "$$\n",
    "W := W - \\alpha \\frac{\\partial J(W)}{\\partial W}\n",
    "$$\n",
    "\n",
    "Таким образом делается это в несколько шагов:\n",
    "- Нам нужно, чтобы все нейроны получили информацию о градиентах, делается это методом `.backward()` тензора `loss_value`, так как градиент рассчитывается на основе функции потерь;\n",
    "- Нужно обновить веса в каждом нейроне, для этого:\n",
    "    - Создадим объект алгоритма оптимизации, мы раньше использовали Gradient Descent, здесь мы используем его модификацию - Stochastic Gradient Descent (SGD). Хорошее описание различных методов оптимизации есть [здесь](https://ruder.io/optimizing-gradient-descent/).\n",
    "    - Чтобы он знал, какие параметры оптимизировать, во время создания объекта оптимизатора передадим ему параметры модели;\n",
    "    - После вычисления loss распространим градиент и выполним шаг оптимизатора;\n",
    "    - Обнулим градиенты на слоях, потому что с каждым вызовом `.backward()` градиенты складываются.\n",
    "\n",
    "Давайте проверим это, сделав один шаг обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oP8NG-U-NUph"
   },
   "outputs": [],
   "source": [
    "# TODO - создайте модель нейросети из одного слоя и одного нейрона\n",
    "#   подготовьте данные и получите предсказание\n",
    "def create_model():\n",
    "    torch.manual_seed(RANDOM_STATE)\n",
    "    return None\n",
    "\n",
    "def predict(model, X):\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 4004,
     "status": "ok",
     "timestamp": 1602520181726,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "DljrV8TtSLfw",
    "outputId": "12a64f5e-b441-4ddd-e577-607b16b59b7b"
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "model = create_model()\n",
    "# Создадим объект Stochastic Gradient Descent\n",
    "optimizer = torch.optim.SGD(\n",
    "    params=model.parameters(),  # Параметры модели\n",
    "    lr=0.01                    # Коэффициент обучения\n",
    ")\n",
    "loss_op = nn.MSELoss()\n",
    "\n",
    "X_tnsr = torch.tensor(X_data).float()\n",
    "# Метод .view() - аналог np.reshape()\n",
    "y_true_tnsr = torch.tensor(y_data).float().view((X_tnsr.shape[0], -1))\n",
    "\n",
    "y_pred_tnsr = predict(model, X_tnsr)\n",
    "loss_value = loss_op(y_pred_tnsr, y_true_tnsr)\n",
    "print(loss_value)\n",
    "\n",
    "# Обнуляем градиенты в модели\n",
    "optimizer.zero_grad()\n",
    "# Делаем распространение градиентов\n",
    "loss_value.backward()\n",
    "# Делаем шаг оптимизации - обновление весов\n",
    "optimizer.step()\n",
    "\n",
    "y_pred_tnsr_new = predict(model, X_tnsr)\n",
    "loss_value_new = loss_op(y_pred_tnsr_new, y_true_tnsr)\n",
    "print(loss_value_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 596
    },
    "executionInfo": {
     "elapsed": 3981,
     "status": "ok",
     "timestamp": 1602520181727,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "d_RqbWGKWjk8",
    "outputId": "35b98ae6-ee85-4fc1-e724-f51cbda042eb"
   },
   "outputs": [],
   "source": [
    "# Отобразим данные\n",
    "\n",
    "y_pred_0 = y_pred_tnsr.detach().numpy()\n",
    "y_pred_1 = y_pred_tnsr_new.detach().numpy()\n",
    "\n",
    "plt.scatter(X_data, y_data)\n",
    "plt.plot(X_data, y_pred_0, 'b--', label='Шаг 0')\n",
    "plt.plot(X_data, y_pred_1, 'g--', label='Шаг 1')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bk1G8X35VcI5"
   },
   "source": [
    "Как видно по числам и графику - шаг оптимизации прошел успешно! Линия стала ближе к данным, а значит веса обновляются в верном направлении!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L2W4o1AUNQe9"
   },
   "source": [
    "\n",
    "Теперь давайте представим нашу систему в виде графа вычислений:\n",
    "\n",
    "![граф вычислений](https://docs.google.com/uc?export=download&id=1AMjPwq28O393LevN0E49cKUcqgby5J7-)\n",
    "\n",
    "Здесь\n",
    "* ребра (стрелки) - тензоры,\n",
    "* узлы (прямоугольники) - модули.\n",
    "\n",
    "По сути после каждой операции тензор-результат получает `grad_fn`, по которой можно обратно распространить ошибку и обновить параметры в модуле, если они есть.\n",
    "> Например, в `nn.MSELoss()` нечего обновлять, но при этом он участвует в вычислениях и учитывается, если распространять градиент от `loss_value`.\n",
    "\n",
    "Так мы разобрались, зачем тензору атрибут `grad_fn`. Теперь, что происходит, если на вход `nn.MSELoss()` мы подадим `y_pred_tnsr`, но перед этим сделаем `y_pred_tnsr = y_pred_tnsr.detach()`? \n",
    "\n",
    "А все не сложно: после вычисления, если мы распространим ошибку методом `.backward()`, то `nn.Linear()` не получит обновления параметров, так как `y_pred_tnsr` является связующим, а мы у него удалили информацию о градиенте!\n",
    "\n",
    "Вот так в PyTorch происходит распространение ошибок и оптимизация параметров для минимизации скалярного тензора, от которого делается `.backward()`. Именно поэтому мы делаем обратное распространение от значения функции потерь!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dFM9M2RKlSdU"
   },
   "source": [
    "## Продолжаем обучать нейросеть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4WxdAfXlVzq"
   },
   "source": [
    "Давайте теперь напишем функцию обучения, которой передадим модель, оптимизатор и на выходе хотим получить историю функции потерь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mo-g49Wzn49Y"
   },
   "outputs": [],
   "source": [
    "# TODO - напишите функцию обучения модели \n",
    "#   с учетом переданных параметров и данных\n",
    "# Мы не возвращаем модель, так как параметры обновляются прямо в ней\n",
    "#   (в переданном снаружи объекте)\n",
    "def fit_model(model, optim, loss_op, X, y, n_iter):\n",
    "    loss_values = []\n",
    "\n",
    "    return loss_values        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 596
    },
    "executionInfo": {
     "elapsed": 4874,
     "status": "ok",
     "timestamp": 1602520182673,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "NLcPyL5VIH5L",
    "outputId": "02bab610-d1dd-47cf-f7cc-62f6b14f9151"
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "model = create_model()\n",
    "# Создадим объект Stochastic Gradient Descent\n",
    "optimizer = torch.optim.SGD(\n",
    "    params=model.parameters(),\n",
    "    lr=0.01\n",
    ")\n",
    "loss_op = nn.MSELoss()\n",
    "\n",
    "loss_history = fit_model(\n",
    "    model=model,\n",
    "    optim=optimizer,\n",
    "    loss_op=loss_op,\n",
    "    X=X_data,\n",
    "    y=y_data,\n",
    "    n_iter=100\n",
    ")\n",
    "\n",
    "assert loss_history[-1] < 0.22\n",
    "\n",
    "plt.plot(loss_history)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qg9RN3yRpdl_"
   },
   "source": [
    "### Задание - изучаем коэффициент обучения\n",
    "\n",
    "Проверьте обучение модели при разных `lr` из списка `[0.1, 0.01, 0.001]` и отобразите графики обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vxr1kkcTQDCf"
   },
   "source": [
    "# Что такое batch и как мне предсказать свое значение?\n",
    "\n",
    "> **Batch** - пакет данных. Нейросети по умолчанию работают батчами, то есть пакетами данных.\n",
    "\n",
    "В нашем случае мы подавали на вход модели все имеющиеся данные, поэтому размер батча для нас - это количество записей в наборе. Формат батча обычно описывается определенными символами в зависимости от размерности и количества размерностей. \n",
    "\n",
    "Буквально сейчас мы учили сеть с форматом входного батча NM, где N - количество данных в батче (все данные, 100 записей), M - количество признаков в данных (в нашем случае 1). Выходной батч имел формат N1, то есть N предсказаний (по размеру входа) и 1 колонка с предсказанным значением.\n",
    "\n",
    "По сути сейчас ничего особо нового, просто особенности терминологии, так как ранее мы тоже подавали на вход моделей 2D массивы. В случае работы с изображениями мы узнаем, что батч может иметь больше двух размерностей и не всегда данные подаются разом, так как вряд ли нам хватит памяти, чтобы загрузить миллион изображений в память.\n",
    "\n",
    "Так вот, так как мы знаем, что вход нейросети предполагает определенный формат пакета, то для случая, если, например, мы захотим предсказать значение для некоторого нового вектора признаков (новая запись данных), то нам нужно привести к требуемому формату: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 4847,
     "status": "ok",
     "timestamp": 1602520182675,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "LucIn0M3TQrd",
    "outputId": "8e57eb10-742d-4511-e6de-3f4aa1634ef8"
   },
   "outputs": [],
   "source": [
    "# Возьмем к примеру случай, когда у нас три признака в данных\n",
    "x_new_sample = [1, 2, 3]\n",
    "new_tnsr = torch.tensor(x_new_sample, dtype=torch.float)\n",
    "print(new_tnsr.shape)\n",
    "# Сейчас запись имеет формат M - кол-во признаков\n",
    "# Чтобы привести его к формату NM, мы должны добавить \n",
    "#   еще одну размерность методом .unsqueeze()\n",
    "\n",
    "# Передали аргумент 0, чтобы размерность добавилась в начале\n",
    "new_batch = new_tnsr.unsqueeze(0)\n",
    "print(new_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QKsiBFFgTeBP"
   },
   "source": [
    "После добавления размерности, если данные уже предобработаны (стандартизированы, сгруппированы и т.д.), то можно подавать батч (хоть и из одной записи) на вход модели!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 613
    },
    "executionInfo": {
     "elapsed": 4824,
     "status": "ok",
     "timestamp": 1602520182679,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "Q8C7Zb9wUErA",
    "outputId": "0f158ca5-fd58-4a79-f993-8d2f46bd6dec"
   },
   "outputs": [],
   "source": [
    "x_new = [2]\n",
    "in_data = torch.tensor(x_new, dtype=torch.float)\n",
    "in_data = in_data.unsqueeze(0)\n",
    "\n",
    "y_pred = model(in_data)\n",
    "# Этим принтом увидим, что выход тоже батчем\n",
    "print(y_pred.shape)\n",
    "\n",
    "# Отсоединим градиент и переведем в формат numpy\n",
    "y_pred = y_pred.detach().numpy()\n",
    "# Так как батч имеет единственную запись - заберем данные из него\n",
    "y_pred = y_pred[0]\n",
    "\n",
    "plt.scatter(X_data, y_data)\n",
    "plt.scatter(x_new, y_pred, marker='x', color='r')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bEhbxn-aU4jC"
   },
   "source": [
    "Видите красный крестик? Вот так модель предсказывает новые данные и при этом один (наш красный крестик) достаточно близок к данным.\n",
    "\n",
    "Таким незамысловатым образом мы смогли получить предсказания по нашим новым данным и узнали, что такое батчи в нейросетях."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOLVRQRGrcMS"
   },
   "source": [
    "# Многослойные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kROQymoxre8D"
   },
   "source": [
    "На данный момент мы отлично справляемся с нейросетью, состоящей из одного нейрона! Настало время попробовать сделать многослойную сеть! Самый простой способ - сделать два модуля слоя и выполнить один за другим!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "executionInfo": {
     "elapsed": 4798,
     "status": "ok",
     "timestamp": 1602520182681,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "42jAKlSksoko",
    "outputId": "391c28df-1bc7-4e1c-e270-cbd6c4d4396a"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(RANDOM_STATE)\n",
    "\n",
    "# Делаем два нейрона в первом слое\n",
    "layer1 = nn.Linear(1, 2)\n",
    "# Так как в предыдущем два нейрона, то здесь два входа\n",
    "layer2 = nn.Linear(2, 1)\n",
    "\n",
    "# Данные примера\n",
    "X_sample = torch.tensor([[1], [2], [3]]).float()\n",
    "\n",
    "# Исполняем один за другим\n",
    "l1_data = layer1(X_sample)\n",
    "y_pred_tnsr = layer2(l1_data)\n",
    "\n",
    "# Смотрим на предсказания\n",
    "y_pred_tnsr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mBtf1dWzvmXy"
   },
   "source": [
    "Первый способ объединения модулей в один является использвание [`nn.Sequential`](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html). Принцип работы с ним в том, что он объединяет операции в последовательность:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "executionInfo": {
     "elapsed": 4770,
     "status": "ok",
     "timestamp": 1602520182681,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "xhFv0yMFJ2hD",
    "outputId": "37897310-e66e-4f1a-ec7e-96a65d49ba86"
   },
   "outputs": [],
   "source": [
    "seq_module = nn.Sequential(layer1, layer2)\n",
    "print(seq_module)\n",
    "\n",
    "y_pred_tnsr = seq_module(X_sample)\n",
    "y_pred_tnsr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hKjYpn0Jpe-"
   },
   "source": [
    "\n",
    "Другим способом является написание класса модели, который наследуется от `nn.Module`. Посмотрим, как это делается:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vlmrWNHIwaoe"
   },
   "outputs": [],
   "source": [
    "class MyLinearModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        # На этой строке вызывается конструктор класса, от которого наследуемся\n",
    "        #   Она нужна, чтобы корректно настроить класс\n",
    "        super().__init__()\n",
    "        torch.manual_seed(RANDOM_STATE)\n",
    "\n",
    "        self.layer1 = nn.Linear(1, 2)\n",
    "        self.layer2 = nn.Linear(2, 1)\n",
    "    \n",
    "    # Метод, который нужно написать, чтобы вызов работал!\n",
    "    def forward(self, X):\n",
    "        l1_data = self.layer1(X)\n",
    "        y_pred = self.layer2(l1_data)\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "executionInfo": {
     "elapsed": 4734,
     "status": "ok",
     "timestamp": 1602520182687,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "bWxHg6XUwxOC",
    "outputId": "3333c866-ce27-47e7-9ea2-c04440b9e95a"
   },
   "outputs": [],
   "source": [
    "model = MyLinearModel()\n",
    "# При отображении показываются все слои внутри модели\n",
    "print(model)\n",
    "\n",
    "# Вот именно в этот момент происходит вызов метода forward() класса\n",
    "y_pred_tnsr = model(X_sample)\n",
    "y_pred_tnsr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6OvR_7bYxNhL"
   },
   "source": [
    "Таким образом создается класс, который содержит всем необходимые слои (можно даже использовать `nn.Sequential` и другие вспомогательные классы внутри) и в нем пишется метод `forward()`, в котором прописываются действия со слоями. Потом объект этого класса можно просто вызывать и получать результат метода `forward()`! Отличная инкапсуляция!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Lzc6KUtx_pE"
   },
   "source": [
    "## Задание - обучение многослойной нейронной сети\n",
    "\n",
    "Самое время обучить модель и понять, лучше или хуже она работает, чем однослойная с одним нейроном:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "juU6SjWbyN2V"
   },
   "outputs": [],
   "source": [
    "# TODO - обучите многослойную сеть и отобразите историю обучения \n",
    "#   и предсказания обученной модели\n",
    "model = MyLinearModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WQavJdXGy2UH"
   },
   "source": [
    "Подумайте над идеей того, что модель теперь состоит из двух слоев и суммарно трех нейронов, но при этом характер предсказания (прямая линия) не изменился. Таким образом, нейронная сеть из слоев с линейной активацией не даст ничего, кроме линейной модели!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6REEoV3WuXv"
   },
   "source": [
    "# Как оценить работу нейросети?\n",
    "\n",
    "Нейросеть = еще один вид модели машинного обучения. При том, что мы решали задачу регрессии, то и все метрики, которые мы использовали для моделей регрессии, применимы здесь!\n",
    "\n",
    "Аналогично, сейчас рассмотрим задачу классификации, но и там такие же принципы для оценки и метрики."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7QaCu3t9jkfp"
   },
   "outputs": [],
   "source": [
    "# TODO - напишите функцию оценки работы модели по метрике R2 \n",
    "#   (не забудьте импорт нужной функции из sklearn)\n",
    "\n",
    "def evaluate_r2(model, X, y):\n",
    "    return r2_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 4640,
     "status": "ok",
     "timestamp": 1602520182701,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "q7EChNnYg-5p",
    "outputId": "4ac00ffc-5280-499d-8c38-a96b340b04a5"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2_value = evaluate_r2(model, X_data, y_data)\n",
    "print(f'R2: {r2_value}')\n",
    "\n",
    "assert np.isclose(r2_value, 0.79912)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2KGsO_ki0DVV"
   },
   "source": [
    "# Я выбираю нелинейность!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8vgsDnd0Gg_"
   },
   "source": [
    "Для работы с данными, в которых нам нужно определить линейные зависимости, достаточно нейросети без активации (или просто нейрона). Теперь, во-первых, давайте начнем работать с данными, которые имеют явную нелинейность, а во-вторых, посмотрим, как добавить слою функцию активации, чтобы добавить нейросети свойство нелинейности.\n",
    "\n",
    "Вот так будут выглядеть наши данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 614
    },
    "executionInfo": {
     "elapsed": 5433,
     "status": "ok",
     "timestamp": 1602520183527,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "gmu12mNw1YQz",
    "outputId": "0641c992-0449-4081-ec11-93dad850fa55"
   },
   "outputs": [],
   "source": [
    "X_data = np.linspace(2, 10, 100)[:, None]\n",
    "y_data = 1/X_data[:,0]*5 + np.random.normal(size=X_data.shape[0])/7 + 2\n",
    "# y_data = (-1)*X_data[:,0]**2+(10)*X_data[:,0] + np.random.normal(size=X_data.shape[0]) + 5\n",
    "\n",
    "# Посмотрим на данные\n",
    "plt.scatter(X_data[:,0], y_data)\n",
    "plt.grid(True)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6H56vfhJ1mHl"
   },
   "source": [
    "Отлично, линейная модель тут уже вряд ли справится, нам нужно научиться добавлять слоям нелинейную активацию!\n",
    "\n",
    "Давайте начнем с одного слоя и сделаем ему функцию активации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "executionInfo": {
     "elapsed": 5406,
     "status": "ok",
     "timestamp": 1602520183528,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "KgUDAdmY14PD",
    "outputId": "afc16f30-3fba-4219-be5d-fc3b7bedf28c"
   },
   "outputs": [],
   "source": [
    "# Создаем слой и задаем свой вес\n",
    "layer = nn.Linear(1, 1, bias=False)\n",
    "layer.weight.data.fill_(10)\n",
    "# Создаем модуль сигмоиды\n",
    "activation_func = nn.Sigmoid()\n",
    "\n",
    "# Данные для примера\n",
    "X_sample = torch.tensor([[-10], [0], [10]]).float()\n",
    "print(f'Input: {X_sample}')\n",
    "\n",
    "# Исполняем вычисления результатов слоя\n",
    "layer_result = layer(X_sample)\n",
    "# Исполняем модуль сигмоиды\n",
    "act_result = activation_func(layer_result)\n",
    "\n",
    "print(f'Layer result: {layer_result}')\n",
    "print(f'Activation result: {act_result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nybLqrz22Qe"
   },
   "source": [
    "Вот таким несложным образом можно добавить в нейросеть нелинейность. Можно создать модуль и исполнять его.\n",
    "\n",
    "Другим более предпочтительным способом является не создание модуля, а просто исполнение функции сигмоиды:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "executionInfo": {
     "elapsed": 5380,
     "status": "ok",
     "timestamp": 1602520183529,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "K56cKQMG3M3W",
    "outputId": "a9c58ba3-427e-44e2-bed4-7a4f755a7983"
   },
   "outputs": [],
   "source": [
    "layer_result = layer(X_sample)\n",
    "y_pred = torch.sigmoid(layer_result)\n",
    "\n",
    "print(f'Input: {X_sample}')\n",
    "print(f'Layer result: {layer_result}')\n",
    "print(f'Activation result: {act_result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HmLXxpOp3jIN"
   },
   "source": [
    "Мы видим аналогичный результат как по значениям, так и по функции `grad_fn`. То есть при написании класса модели можно прямо в методе `forward()` вызывать функцию сигмоиды (или другой функции активации). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ixf3aHTF6hM5"
   },
   "source": [
    "## Задание - нелинейная сеть\n",
    "\n",
    "Реализуйте код двуслойной сети по архитектуре `[2, 1]`:\n",
    "- 2 нейрона в скрытом слое;\n",
    "- 1 нейрон в выходной слое.\n",
    "\n",
    "Скрытый слой должен иметь сигмоиду в качестве активации, выходной слой должен иметь линейную активацию (без активации). После написания обучите модель и посмотрите на предсказания модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3VNwBxxJ7KXJ"
   },
   "outputs": [],
   "source": [
    "# TODO - реализуйте модель нейронной сети с нелинейностью, \n",
    "#   обучите и оцените модель\n",
    "\n",
    "class NonlinearNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(RANDOM_STATE)\n",
    "        # И здесь надо создать слои\n",
    "\n",
    "    def forward(self, X):\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 5336,
     "status": "ok",
     "timestamp": 1602520183532,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "ZYMFY7AuYHN6",
    "outputId": "3d59f141-bf04-4f9c-b3c3-45b841da94cd"
   },
   "outputs": [],
   "source": [
    "model = NonlinearNeuralNetwork()\n",
    "optimizer = torch.optim.SGD(\n",
    "    params=model.parameters(),\n",
    "    lr=0.1\n",
    ")\n",
    "loss_op = nn.MSELoss()\n",
    "\n",
    "loss_history = fit_model(\n",
    "    model=model,\n",
    "    optim=optimizer,\n",
    "    loss_op=loss_op,\n",
    "    X=X_data,\n",
    "    y=y_data,\n",
    "    n_iter=1000\n",
    ")\n",
    "\n",
    "plt.plot(loss_history)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "X_tnsr = torch.tensor(X_data).float()\n",
    "y_pred_tnsr = model(X_tnsr)\n",
    "y_pred = y_pred_tnsr.detach().numpy()\n",
    "\n",
    "plot_model(X_data, y_pred, y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Icqd4EP8rHG"
   },
   "source": [
    "Отлично! По результатам обучения видно, что модель с нелинейностью может иметь нелинейный характер и описывать нелинейные зависимости. Можете самостоятельно оценить работу по численным показателям и кроссвалидацией постараться сделать модель еще точнее!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "foOFLOPIMv8j"
   },
   "source": [
    "# Нейросеть для классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDSvbB4hgd_R"
   },
   "source": [
    "Думаю, и так понятно, что нейросеть не ограничивается только задачей регрессии, поэтому мы зацепим еще и работу модели для задачи классификации! Создадим данные для задачи классификации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 632
    },
    "executionInfo": {
     "elapsed": 6599,
     "status": "ok",
     "timestamp": 1602520184817,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "2K1Ns790My-H",
    "outputId": "de88b2c5-5140-4ed7-9035-59f8960b8a2c"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification, make_moons\n",
    "\n",
    "X_data, y_data = make_moons(\n",
    "    n_samples=1000,\n",
    "    noise=.1,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "pnts_scatter = plt.scatter(X_data[:, 0], X_data[:, 1], marker='o', c=y_data, s=50, edgecolor='k')\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.grid(True)\n",
    "plt.legend(handles=pnts_scatter.legend_elements()[0], labels=['0', '1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vo5q9Xsgi9G"
   },
   "source": [
    "Отлично! Данные есть, теперь можно переходить к написанию модели и обучению!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ymzjd-kkM7l4"
   },
   "outputs": [],
   "source": [
    "class ClassificationNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(RANDOM_STATE)\n",
    "        self.layer1 = nn.Linear(2, 2)\n",
    "        self.layer2 = nn.Linear(2, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = torch.sigmoid(self.layer1(x))\n",
    "        out = self.layer2(out)\n",
    "        y_prob = torch.sigmoid(out)\n",
    "        return y_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BfsnoP3EkQgB"
   },
   "source": [
    "Небольшая модель сделана! Обратите внимание, что у нас выход - это уже не чистое значение линейного слоя, а функция сигмоиды! Так как задача состоит в бинарной классификации, то нам достаточно сказать степень уверенности и задаться порогом, как обычно.\n",
    "\n",
    "Теперь напишем цикл обучения, чтобы обучить модель, при этом для классификации нам нужна другая функция потерь - воспользуемся [`nn.BCELoss()`](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oyCqdqq3kxbT"
   },
   "outputs": [],
   "source": [
    "# TODO - напишите цикл обучения модели\n",
    "# - Создайте модель\n",
    "# - Задайте оптимизатор (SGD)\n",
    "# - Создайте модуль функции потерь\n",
    "# - Запустите обучение через fit_model() - видали, мы даже функцию не переписывали!\n",
    "# - Отобразите историю обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "executionInfo": {
     "elapsed": 8191,
     "status": "ok",
     "timestamp": 1602520186469,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "lW61HcH9mo5S",
    "outputId": "1af788f0-2f68-49cc-b7f0-731c0f48219c"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def show_classification_report(model, X, y):\n",
    "    X_tnsr = torch.tensor(X).float()\n",
    "    y_pred_prob = model(X_tnsr).detach().numpy()\n",
    "\n",
    "    y_pred = y_pred_prob > 0.5\n",
    "\n",
    "    rep = classification_report(y, y_pred)\n",
    "    print(rep)\n",
    "\n",
    "show_classification_report(model, X_data, y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TETFCFIAlGa_"
   },
   "source": [
    "Теперь еще напишем метод визуализации данных, чтобы посмотреть на пространство принятия решений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 614
    },
    "executionInfo": {
     "elapsed": 8179,
     "status": "ok",
     "timestamp": 1602520186470,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "zgytGbA1N4Zj",
    "outputId": "24d5f795-eb8a-4707-d02e-6e168ba3a32e"
   },
   "outputs": [],
   "source": [
    "def plot_2d_decision_boundary(model, X, y):\n",
    "    x1_vals = np.linspace(X[:,0].min()-0.5, X[:,0].max()+0.5, 100)\n",
    "    x2_vals = np.linspace(X[:,1].min()-0.5, X[:,1].max()+0.5, 100)\n",
    "    xx1, xx2 = np.meshgrid(x1_vals, x2_vals)\n",
    "\n",
    "    X_tnsr = torch.tensor(np.c_[xx1.ravel(), xx2.ravel()]).float()\n",
    "    y_pred = model(X_tnsr).detach()\n",
    "    y_pred = y_pred.reshape(xx1.shape)\n",
    "\n",
    "    plt.contourf(xx1, xx2, y_pred)\n",
    "    pnts_scatter = plt.scatter(X[:, 0], X[:, 1], c=y, s=30, edgecolor='k')\n",
    "    plt.xlabel(\"$x_1$\")\n",
    "    plt.ylabel(\"$x_2$\")\n",
    "    plt.grid(True)\n",
    "    plt.legend(handles=pnts_scatter.legend_elements()[0], labels=['0', '1'])\n",
    "    plt.show()\n",
    "\n",
    "plot_2d_decision_boundary(model, X_data, y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-WSS6aA_lMjd"
   },
   "source": [
    "Хммм, явный клиничейский случай недообучения! Модель не может разделить столь нелинейные данные - давайте добавим модели сложности: три слоя и больше нейронов в слое!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SYLB-XHQmJT7"
   },
   "outputs": [],
   "source": [
    "class ClassificationNNv2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        HIDDEN = 20\n",
    "\n",
    "        torch.manual_seed(RANDOM_STATE)\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(2, HIDDEN),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(HIDDEN, HIDDEN),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(HIDDEN, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.layers(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LXGPLcYRmM8e"
   },
   "outputs": [],
   "source": [
    "# TODO - обучите модель снова и посмотрите на показатели (убедитесь, что лучше не стало...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ICsgLKMon5ys"
   },
   "source": [
    "Теперь давайте попробуем поменять функции активации слоев (сигмоида выхода - не функция активации слоя!) с сигмоиды на гиперболический тангенс (в английском назван tanh). Найдите в документации PyTorch соответствующую функцию и обучите третью версию модели. Оцените, насколько изменился характер предсказания модели!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C1jAZvown3_9"
   },
   "outputs": [],
   "source": [
    "# TODO - замените функцию активации слоев и обучите модель, сделайте выводы\n",
    "class ClassificationNNv3(nn.Module):\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.layers(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbcTeCrXXbme"
   },
   "source": [
    "# Как сохранить модель?\n",
    "\n",
    "Один из актуальных вопросов - а как мне сохранить модель, чтобы сохранить результаты обучения? Мне же не придется обучать модель каждый раз заново? Ответ, конечно нет! И для этого PyTorch имеет очень простой функционал!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IYzqGY7xX3n-"
   },
   "outputs": [],
   "source": [
    "# Задаем путь, по которому хотим сохранить параметры модели\n",
    "SAVE_PATH = 'my_model.pth'\n",
    "# Вызываем функцию сохранения\n",
    "# Сохраняем параметры модели!\n",
    "torch.save(model.state_dict(), SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKZjHQ8AYPDP"
   },
   "source": [
    "После сохранения в файловой системе должен появиться файл с названием модели! Вот так можно в файл перенести параметры. А как загрузить их?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 614
    },
    "executionInfo": {
     "elapsed": 8102,
     "status": "ok",
     "timestamp": 1602520186474,
     "user": {
      "displayName": "Алексей Девяткин",
      "photoUrl": "",
      "userId": "11945040185410340858"
     },
     "user_tz": -180
    },
    "id": "dDS8pwxwXbTQ",
    "outputId": "2d33eabb-29e7-492e-9472-3225dbf4900e"
   },
   "outputs": [],
   "source": [
    "loaded_state_dict = torch.load(SAVE_PATH)\n",
    "\n",
    "model = ClassificationNN()\n",
    "model.load_state_dict(loaded_state_dict)\n",
    "\n",
    "plot_2d_decision_boundary(model, X_data, y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kENURkm_Yti9"
   },
   "source": [
    "Отлично! Мы создали новую модель, загрузили сохраненные параметры и все работает! В этом подходе нужно учитывать следующую особенность, если поменяется архитектура модели, то параметры не смогут загрузиться. В остальном можно таким образом переносить обученную модель и использовать ее где угодно (где есть код, чтобы создать объект модели)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKxUIUKbZE1G"
   },
   "source": [
    "# Результаты\n",
    "\n",
    "Мы рассмотрели очень серьезную тему, нейронные сети и применение фреймворка PyTorch! Тем не менее были пропущены такие вещи, как регуляризация нейросетей с помощью `nn.Dropout`, необходимость предобработки данных перед подачей на вход нейросети и другие.\n",
    "\n",
    "Сам по себе фреймворк очень мощный, поэтому мы многое узнаем из новых практик, но главное - вы все можете узнать и попробовать сами! Главное, не бойтесь пробовать и испытывать!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4qYwuyEoqmu"
   },
   "source": [
    "# Выводы - задание\n",
    "\n",
    "Вопрооооосики!\n",
    "\n",
    "1. Из каких частей состоит нейрон? \n",
    "2. Может ли нейрон быть без функции активации? Что из этого получится (какой характер сети)? \n",
    "3. Можно ли в качестве функции активации выбрать кусочно-линейную функцию с разрывами?\n",
    "4. Почему лучше не использовать функцию параболы в качестве функции активации? \n",
    "5. Какие слои нейросети можно назвать \"скрытыми\"? \n",
    "6. Сколько скрытых слоёв может быть в нейросети?\n",
    "7. Можно ли использовать разные функции активации для нейронов в одном слое? \n",
    "8. Как называется процесс, когда функция предсказания работает от слоя к слою? \n",
    "9. Какое процесс описывает получение ошибки предсказания? \n",
    "10. Почему изначально значения весов устанавливаются случайным образом? \n",
    "11. Зачем нужна выборка-валидация? В чём отличие от выборки-теста? \n",
    "12. В чём разница между стохастическим градиентным спуском и полным градиентным спуском?\n",
    "13. Что такое регуляризация? Зачем она нужна? "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Pr6_NeuralNetworks.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
